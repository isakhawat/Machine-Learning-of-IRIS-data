{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML for Diabetes (20+ algorithm applied) .ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQEDg8mcQh-k",
        "colab_type": "text"
      },
      "source": [
        "# **Families of ML algorithms**\n",
        "\n",
        "There are several categories for machine learning algorithms, below are some of these categories:\n",
        "\n",
        "**Linear**\n",
        "\n",
        "1.Linear Regression\n",
        "\n",
        "2.Logistic Regression\n",
        "\n",
        "3.Support Vector Machines\n",
        "\n",
        "**Tree-Based**\n",
        "\n",
        "1. Decision Tree\n",
        "\n",
        "2.Random Forest\n",
        "\n",
        "3.GBDT\n",
        "\n",
        "KNN\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "**And if we want to categorize ML algorithms with the type of learning, there are below type:**\n",
        "\n",
        "**Classification**\n",
        "\n",
        "k-Nearest Neighbors\n",
        "LinearRegression\n",
        "SVM\n",
        "DT\n",
        "NN\n",
        "\n",
        "**clustering**\n",
        "\n",
        "1.K-means\n",
        "\n",
        "2.HCA\n",
        "\n",
        "3.Expectation Maximization\n",
        "\n",
        "**Visualization and dimensionality reduction:**\n",
        "\n",
        "Principal Component Analysis(PCA)\n",
        "Kernel PCA\n",
        "Locally -Linear Embedding (LLE)\n",
        "t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "**Association rule learning**\n",
        "\n",
        "Apriori\n",
        "Eclat\n",
        "\n",
        "**Semisupervised learning**\n",
        "\n",
        "**Reinforcement Learning**\n",
        "\n",
        "Q-learning  \n",
        "\n",
        "**Batch learning & Online learning**\n",
        "Ensemble Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4irQVtS9fDi",
        "colab_type": "text"
      },
      "source": [
        "**source**\n",
        "\n",
        "https://www.kaggle.com/ranjeetjain3/visualization-machine-learning-deep-learning#-Description\n",
        "\n",
        "# **import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsl1a9GeDlqK",
        "colab_type": "code",
        "outputId": "5e72f1d0-9280-458f-e029-f058876e9969",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        }
      },
      "source": [
        "#@title All library \n",
        " #%matplotlib inline = show plots in Jupyter Notebook browser\n",
        "%matplotlib inline\n",
        "\n",
        " #foundational package for scientific computing\n",
        "import numpy as np\n",
        "\n",
        "#collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
        "import pandas as pd\n",
        "\n",
        "#collection of functions for scientific and publication-ready visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Visualization\n",
        "import seaborn as sns\n",
        "\n",
        "#collection of functions for scientific computing and advance mathematics\n",
        "import scipy as sp\n",
        "\n",
        "#collection of plot\n",
        "import plotly.offline as py\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "import plotly.graph_objs as go\n",
        "from plotly import tools\n",
        "init_notebook_mode(connected=True)  \n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "#collection of machine learning algorithms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Common Model Helpers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "import pylab as pl\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import  accuracy_score\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as  lgb\n",
        "#from xgboost.sklearn import XGBClassifier\n",
        "#from catboost import CatBoostClassifier\n",
        " \n",
        " \n",
        "  \n",
        "  \n",
        "  \n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvFeQX-QO5Kd",
        "colab_type": "text"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrTf9cwkO2nY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data =  pd.read_csv('diabetes.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCvI23FHO2sY",
        "colab_type": "code",
        "outputId": "df39f599-9da0-4aad-acd7-74fe8e14d321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpH8wzglO2zH",
        "colab_type": "code",
        "outputId": "a1c9a2be-6292-4ee1-810d-5d73ddbba6c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcrwje0qO2vz",
        "colab_type": "code",
        "outputId": "9233f9a6-dec3-4c75-88e1-9680d9e4d818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            "Pregnancies                 768 non-null int64\n",
            "Glucose                     768 non-null int64\n",
            "BloodPressure               768 non-null int64\n",
            "SkinThickness               768 non-null int64\n",
            "Insulin                     768 non-null int64\n",
            "BMI                         768 non-null float64\n",
            "DiabetesPedigreeFunction    768 non-null float64\n",
            "Age                         768 non-null int64\n",
            "Outcome                     768 non-null int64\n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inByr0jsPcPi",
        "colab_type": "text"
      },
      "source": [
        "# **Missing data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A95x_nXDvMw",
        "colab_type": "code",
        "outputId": "adad5309-4f07-48b0-ee97-7238112a0b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "\n",
        "#Amount of data missing by columns\n",
        "missing = [(c, data[c].isna().mean()*100) for c in data]\n",
        "missing = pd.DataFrame(missing, columns=[\"column_name\", \"percentage\"])\n",
        "missing = missing[missing.percentage > 0]\n",
        "display(missing.sort_values(\"percentage\", ascending=False))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>column_name</th>\n",
              "      <th>percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [column_name, percentage]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVxniKq2QfnH",
        "colab_type": "text"
      },
      "source": [
        "# **Data Cleaning**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsWAvf9lQgIO",
        "colab_type": "code",
        "outputId": "e72ed6e0-db60-480c-aa77-833f7ee3f01e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print('Diabetes columns with null values: {} \\n' .format( data.isnull().sum()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Diabetes columns with null values: Pregnancies                 0\n",
            "Glucose                     0\n",
            "BloodPressure               0\n",
            "SkinThickness               0\n",
            "Insulin                     0\n",
            "BMI                         0\n",
            "DiabetesPedigreeFunction    0\n",
            "Age                         0\n",
            "Outcome                     0\n",
            "dtype: int64 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4y6KUlrQl_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJUkLVypRSX-",
        "colab_type": "text"
      },
      "source": [
        "# **Visualization**\n",
        "\n",
        "some graphical representation of information and data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq69Vq9vYRxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corelation = data.corr()\n",
        "data = [go.Heatmap(z = np.array(corelation.values),\n",
        "                   x = np.array(corelation.columns),\n",
        "                   y = np.array(corelation.columns),\n",
        "                     colorscale='Blackbody',)\n",
        "       ]\n",
        "py.iplot(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s2hVbjXQmHK",
        "colab_type": "code",
        "outputId": "9e0dd37a-8af7-4c6e-9e73-41cb4754f26f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        " sns.countplot(x=\"Outcome\", data=data)  # How many people survived\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fab0253fc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKnbVeRcQmK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#graph individual features by survival\n",
        "fig, saxis = plt.subplots(2, 2,figsize=(16,12))\n",
        "\n",
        "sns.countplot(x='Outcome', hue=\"Pregnancies\", data=data,ax = saxis[0,0])   \n",
        "#sns.countplot(x='Outcome', hue=\"Glucose\", data=data,ax = saxis[0,1])\n",
        "#sns.countplot(x=\"Outcome\", hue=\"BloodPressure\", data=data, ax = saxis[1,0])\n",
        "sns.countplot(x=\"Outcome\", hue=\"SkinThickness\", data=data, ax = saxis[1,1])\n",
        "sns.countplot(x=\"Outcome\", hue=\"Insulin\", data=data, ax = saxis[1,1])\n",
        "#sns.countplot(x=\"Outcome\", hue=\"BMI\", data=data, ax = saxis[1,1])\n",
        "#sns.countplot(x=\"Outcome\", hue=\"DiabetesPedigreeFunction\", data=data, ax = saxis[1,1])\n",
        "#sns.countplot(x=\"Outcome\", hue=\"Age\", data=data, ax = saxis[1,1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoDDMQkFTZ-P",
        "colab_type": "code",
        "outputId": "01ee1c88-fcfb-4c42-8b9d-710fa2cafd44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        }
      },
      "source": [
        "#graph individual features by survival\n",
        "fig, saxis = plt.subplots(2, 2,figsize=(16,12))\n",
        "sns.countplot(x='Outcome', hue=\"Pregnancies\", data=data,ax = saxis[0,0])\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7faafaf40940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7sAAAKvCAYAAACrqtjPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7AIAADCO2AUAAGAcsQsAAMA4/wv7MgSupxZM6wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x864 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TmsqlaPQmEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot distributions of age of passengers who survived or did not survive\n",
        "a = sns.FacetGrid( data, hue = 'Outcome', aspect=4 )\n",
        "a.map(sns.kdeplot, 'Age', shade= True )\n",
        "a.set(xlim=(0 , data['Age'].max()))\n",
        "a.add_legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkx_AEeOU3cs",
        "colab_type": "code",
        "outputId": "3bfcc656-8a26-4057-a9e3-27b27ac3bc97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "#plot distributions of age of passengers who survived or did not survive\n",
        "a = sns.FacetGrid( data, hue = 'Outcome', aspect=4 )\n",
        "a.map(sns.kdeplot, 'Glucose', shade= True )\n",
        "a.set(xlim=(0 , data['Glucose'].max()))\n",
        "a.add_legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7faaf87d8630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4oAAADQCAYAAABBcJ/0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd8VfX9x/HX947c7L3IIgkkrLBR\nZFgHoKio4Gi1trbO2l3rz1Y7ra21SzustVqr1lGtWlfde6GykT0CBEISsvfOvd/fHycWtIoRgZub\n+34+Hudx17mXzwXuPfd9vstYaxERERERERF5jyvYBYiIiIiIiMjgoqAoIiIiIiIi76OgKCIiIiIi\nIu+joCgiIiIiIiLvo6AoIiIiIiIi76OgKCIiIiIiIu+joCgiIiIiIiLvo6AoIiIiIiIi76OgKCIi\nIiIiIu/jCXYBn8T8+fPts88+G+wyRERERERk6DPBLiCYQqpFsa6uLtgliIiIiIiIDHkhFRRFRERE\nRETk0FNQFBERERERkfdRUBQREREREZH3UVAUERERERGR9wmpWU9FREQOhkDAUt3aRVVzFz6Pizif\nlxifm9hIDz6PO9jliYiIBJ2CooiIDFnlDR1sqGqhvKGDXe9t9R3sbuykxx/40Od43YYYn4dYn4eM\n+EimDk/675Ya6zvM70BERCQ4jLU22DUM2LRp0+zy5cuDXYaIiAxS1lo2V7fy3Lpqnl1fxcaq1v8+\nFh3hJiM+krQ4H+lxPtLjIkmJjcAfsHT2+Ons7d/6r3f1+Klu7WJ7bTt9AedYOTwlmmnDk5k6PIlp\n+UkUpcdiTFgvsyUiMpSF9Re8WhRFRCSkBQKWVeVNPL9+D8+s28Ouhg4MUJwRx3nT8xgzLJ6MuEhi\nfO4DCnU9fQHK6tvZvKeVLdWtvLBxD/9euRuA/JRozp6Wy5lTcshMiDzI70xERCR41KIoIiIhqbmz\nl7sWl3Hvkp3UtnbjcRnGZcVzRL7T4pcYHXFI/lxrLXtauthY1cri0lo2VLXiMvCZojTOnpbL3LHp\nGucoIjI0hHWLooKiiIiElKaOHu5YXMadi3fQ2tXH5LxEZo1IZXJeItERh7+jTHVLF69tqeX1LbXU\nt/eQEOVl0eRsPjstl7FZ8Ye9HhEROWgUFEOFgqKISPhqaO/h729u5663ymjv9nNkQTKLJmeTnxIT\n7NIApwvsuspmXt1cy7KyBvoCluNGpfHtucVMyk0MdnkiIvLJKSiGCgVFEZHwU9fWzd/e2M7db++k\nq8fP9MJkFk3OIS85OtilfaS2rj5e3FjNU2uraOvuU2AUEQlNCoqhQkFRRCR8BAKW+5bs5PpnNtHV\n62dGYQoLJ2eTkzR4A+IHdfb4eX7DHp5aW0VrVx/HFqfx7blFTM5LCnZpIiLy8RQUQ4WCoohIeNhR\n1873H17D0rIGxmcn8OWZ+WQlRgW7rAP2YYHxuycUMyFHLYwiIoOYgmKoUFAUERna/AHL39/czg3P\nb8HjMpx31HCOLU4bMmsVdvb4eWHDHp7sD4xnTsnh+/NHkR6vpTVERAahoXHwOUAKiiIiMihsqW7l\nyofe5d3dzUwdnsSFswpIjjk0S1wEW0dPH4+vruTptVVEeFx8/biRXDS7gEivltUQERlEFBRDhYKi\niMjQ0+sPcMur2/jTS1uJjnDzpZn5zChMGTKtiPtT3dLFfUt2sqyskZykKH50yhhOHJcZFu9dRCQE\nhPWXsYKiiIgEzZ7mLi67dwWry5uYOSKFL83IJz7KG+yyDrt1Fc3c/U4Z5Q2dHFWQzE9OHac1GEVE\ngk9BMVQoKIqIDB3Lyxr46r0rae3u5SufGcFRhSnBLimo/AHLy5uqeWjFbtq7+/jiUcO54sRRxEeG\nX3AWERkkwjoougaykzFmvjFmszGm1Bhz1Yc87jPG/Kv/8SXGmPz++1OMMa8YY9qMMX/+wHNe7X/N\n1f1b+sF4QyIiMrhZa7n3nZ2cc9s7uF2Ga08rCfuQCOB2GeaNzeTGz05i7pgM7nlnJ8f/7lUeX11B\nKJ3UFRGRoeFjg6Ixxg3cDJwEjAXONcaM/cBuFwGN1tqRwO+BX/ff3wX8GPi/j3j586y1k/q3mgN5\nAyIiEjq6+/xc9e+1/OixdYzPTuAXC0vITQ6ddREPh1ifhwtmFfDz00tIiPLy7QdW84Xbl7Ctti3Y\npYmISBgZSIvikUCptXa7tbYHeAA4/QP7nA78o//6w8AcY4yx1rZba9/ECYwiIhLG9jR38blb3+Ff\ny8tZOCmb/zthFDE+T7DLGrQK02K59rQSLpyVz6ryJk78/ev87rnNdPb4g12aiIiEgYEExWygfJ/b\nu/vv+9B9rLV9QDMwkH5Ed/Z3O/2x+Ygp3owxlxpjlhtjltfW1g7gJUVEZLBZVtbAgpveYNOeFi6f\nW8znjsjF5QrroR8D4urvjnrD2ROZUZjCn18pZd6Nr/HypupglyYiIkPcgMYoHiLnWWvHA0f3b1/8\nsJ2stbdZa6dZa6elpaUd1gJFROTTe3x1Befe9g4el4trTyvhyILkYJcUchKjI/jacSP58YKxWAMX\n3rWcr9yznIqmzmCXJiIiQ9RAgmIFkLvP7Zz++z50H2OMB0gA6vf3otbaiv7LVuCfOF1cRURkCLnn\nnZ1854HVFGXE8nONR/zUxg6L51eLxnPuEbm8urmWuTe8xq2vbaPXHwh2aSIiMsQMJCguA4qMMQXG\nmAjgHOCJD+zzBPCl/utnAS/b/UzRZozxGGNS+697gQXAuk9avIiIDE7WWm5+pZQfP7aOSXmJXDV/\nDLEaj3hQeNwuTpuUzW/PmsjYrHiuf2YTJ//xDZbuaAh2aSIiMoQMaB1FY8zJwB8AN3CHtfY6Y8y1\nwHJr7RPGmEjgHmAy0ACcY63d3v/cMiAeiACagBOAncDrgLf/NV8Evmut3e8Ifa2jKCIy+Flr+dUz\nm7j19e3MGpHCZceOwOMK5kiHoW35zgbufnsnta3dnDklhx+cPJqUWF+wyxIRGQrCejD9gILiYKGg\nKCIyuPkDlh8+upYHlpUzb2wGX56Zj+vD5yqTg6ir189jqyt4ak0VMT4P35s/inOPyNOEQSIin05Y\nf4kqKIqIyEHR3efnu/96l6fWVrFwUjafnZbDR0xoLYdIRWMndyzewYaqFiblJvKLhSWUZCcEuywR\nkVAV1gcxBUUREfnUOnr6+Mo9K3hjax3nTc9jwYSsYJcUtqy1vFlax31LdtHa1cv5M/K54oRi4iK9\nwS5NRCTUKCiGCgVFEZHBp6Onjy/dsZQVOxu5+OhCjhuVHuySBGjr7uPB5eW8uKGa1DgfP14wllMn\nDFMrr4jIwIX1F6ZmFxARkQPW1evnkruXs2JnI984rkghcRCJ9Xm4cFYBP19YQpzPw7fuX8UX/r6E\n7bVtwS5NRERCgIKiiIgckF5/gK//cyWLS+u59DMjmDEiJdglyYcYkRbLz08v4YKZ+aze1cSJf3id\nG5/fTFfvficaFxGRMKegKCIin5g/YPnug+/y0sYaLpiVzzHFacEuSfbD5TKcMC6T3509kSMLUvjT\ny6XMu/E1XtlcE+zSRERkkFJQFBGRTyQQsFz9yBr+824lnz8yjxPGZga7JBmgxOgIvnHcSH50yhgC\nFi64cxmX3buCyqbOYJcmIiKDjIKiiIgMmLWWa5/cwIPLd3PG5GxOnajZTUPRuKwEfnXGeD43LZeX\nN9Yw54bX+Nvr2+n1B4JdmoiIDBIKiiIiMmC/e34zd71VxsklmZw1NSfY5cin4HG7WDg5m9+eNYEx\nw+K47umNLPjTmywvawh2aSIiMggoKIqIyIDc/EopN7+yjTmj0/nCUcO1zMIQkR4fyf+dMIor5hVT\n397NWX99mysfepeG9p5glyYiIkHkCXYBIiIy+N23ZCe/fW4zs0akcOGsAoXEIcYYw7T8ZEqyE3h0\nVQWPrKrg+Q3VXH3SaD47LReXS//eIiLhRi2KIiKyX69squHHj61jcm4ilx07QqFhCIv0ujn3yDyu\nXzSerMRIrnpkLWf+9S02VLYEuzQRETnMFBRFROQjrato5mv/XMnwlBi+NacIj0uHjXCQmxzNj08Z\ny1ePGcG22jYW3PQG1/5nA23dfcEuTUREDhMd8UVE5ENVNHVywV3LiIlwc+WJo4j0uoNdkhxGxhg+\nU5zGDWdN4vjR6dy5eAfH/+5Vnlu/J9iliYjIYaCgKCIi/6Olq5cL7lxKe3cf3ztxNEnREcEuSYIk\nNtLDRbMLufb0EqIi3HzlnhV88/5VmuxGRGSIU1AUEZH36ekL8NV7VrCttp3L5xaTmxwd7JJkEBiZ\nHssvFpZw9tQcnllbxdwbX+OpNVXBLktERA4RBUUREfkvay0/eHQti7fVc8nRhZRkJwS7JBlEPC4X\nZ0zJ4bpF40mM8vL1f67kq/euoLa1O9iliYjIQaagKCIi/3XTy6U8vGI3Z07J5pjitGCXI4NUXnI0\n155ewjlH5PLixmrm3vgaj62qwFob7NJEROQgUVAUEREAHlm5mxtf2MLRRamcOSUn2OXIIOd2GU6f\nlM31iyaQHufjO/9azaX3rKC+Ta2LIiJDgYKiiIiwvKyB7z28hpKseC49uhBjtFaiDEx2UhTXnDqO\n86bn8cqmGub/8Q3e3FoX7LJERORTUlAUEQlzlU2dXHbvClJjfXx7bjEetw4N8sm4XIYFE7L4xcIS\nIjwuvvD3JVz/9EZ6+gLBLk1ERA6Qfg2IiISxrl4/l96znPZuP1ecUEyszxPskiSEDU+J4bqFJcwd\nk86tr2/njFsWs6OuPdhliYjIAVBQFBEJU9Zarvr3GtZXtPD140aSk6RlMOTT83ncXDS7kO/OLWZn\nXQcn/+kNHlperoluRERCjIKiiEiY+tsb23lsdSVnT8tl6vCkYJcjQ8wRBclcf8Z4ClJiuPLhNXzz\n/lU0d/YGuywRERkgBUURkTD06uYafvXMJqYXJLNwUlawy5EhKiXWxw9PHsPnpuXy9NoqTvvzm2za\n0xLsskREZAAUFEVEwsz22ja+ef8qcpOiueyYEZrhVA4pl8uwcHI2P1kwjpbOXhbd/BaPr64Idlki\nIvIxFBRFRMJIa1cvl9y9HIArTigm0usOckUSLkZlxnHdovEMT4nm2w+s5tr/bKDXr1lRRUQGK01v\nJyISJgIBy7cfWM2OunZ+ePIY0uIig12ShANrcfe24u2sI6+rjt+X1PCuZwu1Sx7k9Q1dzB4WwOdv\nh74e8HdDX7dz6e/tv94DgT7w+MAT6WzeqP+9jEqEmDSISXcuY9P23o5OBpdOioiIfBIKiiIiYeIP\nL27h5U01XDAzn7FZCcEuR4YYV18XkS07iGrZRlTztv7LUiJbynD7u9637xgg4HFR1xFP2fYEspJi\niYuKcEKfLxZcXnB7weVxLo3bCY7+nvdvXc3gr3OCZXcbdDU5ofKDjMsJjMkFkJTfv+1zPTYd1AVb\nROR9FBRFRMLASxur+dPLpRxTnMa8sRnBLkdCnKergdi6VcTVriKmYT1RzaX42isxOEtgWAw9Uen0\nxAyjMfs4+iKT6YtI2Lv5EvF7Y9nR6uK6ZVBbDT+dGckXxno/3ZhZa6GnHboaobPZCY5dTdDZBB11\n0FoNpS9Cex2wz3Id3ihIzIf00ZA+FtLHQNoYJ1iqJVJEwpQJpXWNpk2bZpcvXx7sMkREQsqu+g4W\n3PQGKbE+rjl1HBEeDU+XTyDQR3TTVuLqVhJbu4q42pVEtZYBYI2LrthcumOy6YnOojvG2XqiM7Hu\niAG9fGsP3LAKltXAeWO9XDMzEq/7ELfu+XugrQZaq6B1T/9WCc27nfve4/FB6qj+8DgaMsZD1iSI\nST209YnIYBHWXQ0UFEVEhrCuXj+L/rKYXQ0dXLdwPBnxGpcoH8NaIlu2k1TxKomVrxFXuwJ3XycA\nvREJdCaMpDOhiI7EIjrjC7Fu36f+IwMW7t4ED5XCrGw3f5kXTYIvSL/PejuhuRwad0LTLmjqv+yo\n27tPXBZkTXZC47CJMGwSxKmlXmQIUlAMFQqKIiIDZ63lyofX8PCK3Vx54iim5CUFuyQZpFy9HcRX\nv01SxWskVrxCZLuzfEVXTA7tyWOcYJhQRG/UoR3L92I53PQu5MYb7jgpmoKEQdTts7sNGrdDfSnU\nb4OGbdBcwX+7sMZmOuExZxrkHAHZU8AXF9SSReRTU1AMFQqKIiIDd//SXVz9yFrOmJzN2dNyg12O\nDDIR7ZUk73qOpIpXia9egivQQ8Dtoy25hLaUibSlTqQ3Ku2w17W+Hq7rP9TfckI0M7MH8XQKPR3Q\nsN0JjfWlUL/V6b4KgHHGOuYcsXdLLQaXun6LhBAFxVChoCgiMjBrdjdx5i1vMWZYPN8/cTQuV1gf\n66Sfp6uBlF3PkLrjCeJrlgHQFZNNW8oE2lIn0ZE0GuvyBrlKqGqHa5dBZRtcOzuSz48d2HjHQaG7\nDeo2Q+3mvZc9bc5jvjjInro3OGZPg5iU4NYrIvsT1gdPBUURkSGmsb2HBTe9QXdfgOsWjSc+Mvg/\n/CV4XL1tJJe/SOqOJ0ioehOX7aMrJpuWzBk0Z8ygJ2ZYsEv8UB298OuVsLwGLhwfwQ+P8uEOxRMe\nNgAtlU5grN3khMfGMud+cJbpyD2yPzxOg4wSZ0kQERkMQvBL5+AZUFA0xswH/gi4gduttb/6wOM+\n4G5gKlAPfM5aW2aMSQEeBo4A7rLWfmOf50wF7gKigKeBb9uPKUZBUURk//wBy4V3LWNxaR3XnDaO\nEWmxwS5JgiHgJ7HyNdK2P0rS7pdw+7voiUylJXMGTZkz6Y7NC4l1A/0B+PsGeHwHHJvr5qa50cRF\nDP66P1Zvl9NVtW5Tf4DcDJ0NzmMeH2RN2TvWMecIiM8Kbr0i4WsIfOEcuI8NisYYN7AFmAfsBpYB\n51prN+yzz9eACdbay4wx5wCLrLWfM8bEAJOBEqDkA0FxKfAtYAlOUPyTtfaZ/dWioCgisn+/f2EL\nf3xpKxfNLmDuGM3CGG68nbWklz5Ixtb78bVX0ueNpyVjOk3DZtKZUOQsPB+CnimDW9ZBYaKLO+ZH\nkxsfmu/jI1kL7bV7WxxrtzhBMtDrPB6XBbn7jHUcNtFZ+1FEDrWwDooDGSF+JFBqrd0OYIx5ADgd\n2LDPPqcD1/Rffxj4szHGWGvbgTeNMSP3fUFjzDAg3lr7Tv/tu4GFwH6DooiIfLRXNtfwp5e2cnRR\nKnNGpwe7HDlcrCW+egkZW+4jufw5XIE+2pLHUTPibFrSpoJrEE8GM0An5UNWLPxyeYDTHm3jbydG\nMy0z9N/XfxkDsenOVvAZ5z5/rzNRznvjHMuXwIbHncdcHsgc74xxfK/LanJhSLQSi0joGMi3bDZQ\nvs/t3cD0j9rHWttnjGkGUoA6Plx2/+vs+5rZH7ajMeZS4FKAvLy8AZQrIhJ+yhs6+PYDq8hLjuai\n2QUY/WAc8tw9LaRte4SMLfcR3bKNPm8sDTnzaMyZQ0/M0OuqODEVbpwNP1sK5/6ng18fE8kZxSE0\nyc0n5fZC2ihnG9N/X2ej09r4XpfVVffCsr85j0Ul7zNJzhRnqY7o5KCVLyKhb9CfjrPW3gbcBk7X\n0yCXIyIy6HT1+vnqvSvo81u+M7cYn2cQrT0nB11EewVZG/5O+tYHcPu76IgfQcW4r9CcMQPrHsLB\nCciOhRtmw/Ur4LuvdFHaGOD/jvThCpcTI1FJkDfd2QACfmjatbfVsXYjbH1u7/4JeZA92QmNwyZB\n1iTnNUREBmAgQbEC2HcBrpz++z5sn93GGA+QgDOpzf5eM+djXlNERAbgmifWs66yhStOKCYzITLY\n5cghEtW0haz1t5G64wkMlubMGdTnnURXfEGwSzus4iLg2unw13Xwl9U9bGsK8Pvjo4j2hklY3JfL\nDckFzlY837mvpw3qtzlrOtZvg/Kle7usAiTlO8Exo8TpvppR4kyWEy5hW0QGbCBBcRlQZIwpwAlz\n5wCf/8A+TwBfAt4GzgJe3t8MptbaKmNMizHmKJzJbM4HbjqA+kVEwtqDy8p5YFk5p0/KYtpwdTMb\niuJqlpO17laSK14i4PbRmDuX+ryT6Y1KDXZpQeNxwdfHQ24s3L6hj7Meb+fv86MZFjvEJrk5EBGx\nzmQ3wybuva+7dZ/wuBV2vg3rH937eFRSf2gcD5klTnhMLQavTjyJhLOBLo9xMvAHnOUx7rDWXmeM\nuRZYbq19whgTCdyDM8NpA3DOPpPflAHxQATQBJxgrd1gjJnG3uUxngG+qeUxREQGbl1FM2fe8hZF\nGbFcPX8MrlBcY04+nLUkVrxM9rq/El+7gj5vHA25J9KQOw9/RFywqxtUllc76y3GeA23z49mYrq6\nXg9IT7uznmPjDmfSnMYyaNwJ/m7nceNyWh/TxjjjJNP7L1OKICI6iIWLHFZhfWAdUFAcLBQURUQc\nzR29nHLTG3T0+PnlovEkRGmB7iHBWhIrXyV31Q3ENm6gJzKN+uEn05h9DNat1p2PsrMVrl0Kjd1w\nw3FRLBihz8MBCfihtRIadkBzuTP+sbkcWiqcxwAwkJjnhMbkEZAyor/76wjnfpeCugwpAw6Kxpgc\n4GZgLOACngSutNb27Oc5P7DW/vJTV3mIKCiKiISYQMBy8T+W8frWOn6yYCxFGWphGgriq5eQu+p3\nxNeuoDsqg9rCRTRnzhwSy1scDs3dcN1yWN8Al0/z8a0pEZr992AJ9EFLFTTt3BsgWyqgpRL6uvbu\n5/JC0nBIGeks15GY9/4tMiF470HkwAzoS8Q4XzZLgFustXf2r0N/G9Bgrb1yP89rs9bGHpxSDz4d\nfUREQszNr5Ty8uZaLpiZr5A4BMTWvUvu6htIrHqTXl8ylWMuojHrGAXETyjBB9cdBTetgd8v76a0\n0c9vj40i0qOw+Km5PJCY62z7stZZsqOl0mmJbOnfajfB9lffHyLBCYoJuZA43AmO8VkQNwziMvde\n+gbtb2aR/Tke6LLW3glgrfUbYy4HdhhjdgBjrbXfADDGPAn8DpgPRBljVgPrrbXnGWPOB/4PsMAa\na+0XjTH5wB1AKlALXGCt3WWMuQvoxBn6lw5ciDPvywxgibX2y/1/3gnAzwAfsK3/+W0DeVM6ComI\nhJDXt9Ry4wtbmDUylXljM4JdjnwK0Y2byF19I8m7X6TPG8+e4vNoyJk35Je4OJS8brh8EuTFwV0b\n+yhvbee2E6NJj9YkN4eEMc5ajdHJziQ4+7IWulugrRraavq3amivgep1sO2l/w2S4EzG88Hw+D+X\nmeCNOjzvUWRgxgEr9r3DWttijNnFR+Qta+1VxphvWGsnARhjxgE/AmZaa+uMMe/NUHcT8A9r7T+M\nMRcCfwIW9j+WhBMMT8OZXHQWcDGwzBgzCWet+h8Bc6217caY7wPfBa4dyJtSUBQRCREVTZ1864FV\n5CRHcfHsAnWrC1ER7ZXkrfotqTueIOCJonrE2TTkzSfg0Q/fg8EYOGskZMfA71YFOP2Rdm6fH824\nVI2dO6yMcVoQIxOcGVQ/yFro7YCOBuhs6L+sdy47GqCjHupLnUv/hwzxikyA2EyIy4CYdIjt3z54\nPSYV3BqzKiHheOAha20dgLW2of/+GcAZ/dfvAX6zz3P+Y621xpi1QLW1di2AMWY9kI+zBOFYYHH/\nb4YInFUqBkRBUUQkBHT3+fnqvSvo6Qtw+ZxiIr360RtqXL3tZK+/jWEb/oaxAeryF1Cffyp+r7ra\nHQozhsFvouHaZZazHm/nl0dHsqhYrbWDhjEQEeNsH+zSui9rnbUhO/pDZGfjPoGyHtproa7UCZsf\n1kIJzvIfsRn7BMkMiE1TqJSDaQPOEoH/ZYyJB/JwVn3Yt1vDwZyZrH+aYgL7XH/vtgfwAy9Ya889\nkBdXUBQRCQHX/mcDa3Y38925xQxLVMtTSLEBUrc/xvBVvyGis4bmjBlUF51Db1RasCsb8kYkwO9n\nw29WwuWvdLGi2s+PZ0bic6s1PmQYA744Z0vK3/++vV3Q1dQfJhv3ud7kXG+rgbqtzn0fGSqTP9A6\nmeF0dU3IhvhsZ1xlbCZ4dNJB3ucl4FfGmPOttXf3T2ZzA85SgNuBy4wxLiAbOHKf5/UaY7zW2l7g\nZeBRY8yN1tp6Y0xyf6viWzjr2N8DnAe88Qnqege42Rgz0lpbaoyJAbKttVsG8mQFRRGRQe7fK3Zz\n35JdLJgwjCMKkj/+CTJoxNUsJ3/Zz4ltWEtH/Ah2l3yNzsQP6YYnh0xypDPJzT82wb0bellT6+cv\n86LJidO4xSHHGwne/jGMH6e3c2+A/G+Y7L/sbNwbKjsa9q4t+V8GYtL2CY/ZTohNGu5cJg7XpDxh\npr/75yLgL8aYH+O0ID4N/ADoAXbgtDpuBFbu89TbgDXGmJX9k9lcB7xmjPEDq4AvA98E7jTGXEn/\nZDafoK5aY8yXgfuNMb7+u38EDCgoankMEZFBbGNVC4v+spjC1Fh+cPIY3C61hIQCX9tu8lb+mtSd\nT9HrS6a66BxnqQujcBJMb1fBjashwg1/OD6aY/N0vlw+hrXQ2w7tdf3dX+v7r+9zu63aCZ77ik6B\nxPy94TG1uH8rgsj4YLwTOTBhfdBVUBQRGaSaO3s59aY3ae3q5ZeLxpMYra5Og53xd5O9/lay1/4F\ngLr8BdTlL8C6D+aQFPk0Ktvhl8uhrAW+OSWCb0/16QSMfDrvm+G1Glr37HNZA+3VEPDv3T82E9KK\nIXWUEx7TiiF9rNPdVQabsP5y0Kk0EZFBKBCwXPHgaiqaOvnxKWMVEkNAQuUbFCz9CVGtO2nOOIo9\nxefRF5kS7LLkA7Ji4Hez4JZ18KeVPays8fPH46NIiVJrrxygj5vhNdDnhMbm3f1buXO5e4XTWvme\n2AwYNhEyJziXwyY43Vg1w7UEiYKiiMgg9NfXt/HixhrOnzGcUZlxwS5H9sPbUU3+8l+QuvMpuqOH\nUTblatpTxge7LNmPSA98ZyKMTYJb1vk58aF2fnNMJMcP14yXcgi4PJCQ42z7stYZD9lcDo1lznIg\ntVug9CWw/S2QkQmQOR6GTYLc6ZB3lFoe5bBR11MRkUFmcWkdX/z7EqYXpPDN40dqvcTBKtBH5uZ7\nyFt9IybQQ13B6dQNX4B1q/Vft3o1AAAgAElEQVQ3lOxogd+tcrqinjvGy49mRBLj1WdOgqivG5p2\nQv02aNgGDdudIPneepLJBZA3sz84znDGPeo4caiE9V+sgqKIyCBS1dzJKX96k6gIN784vUTrJQ5S\nsbWrKFzyI2IaN9KaMpGq0V+mNzoj2GXJAer1wz2b4ZFtkBtv+P1xUUzNVKcrGUT8vU6LY80GqNkI\ntRuhq9l5LCoJco+CwmNgxBwFx4MrrP8iFRRFRAaJrl4/n731bbZWt/LzhePJ1nqJg467p5W8Vb8m\nY8v99PmSqBp1Pq3pR+hH2RCxrh5uXAW1XfC1SRF8a6qPCK25KIORtdBSsTc41qyHlkrnsfgcGDnH\n2QqOgajE4NYa2sL6C0BBUURkELDWcsVD7/LIygqumFfMtHytlzjYJJW/RMGSHxHRWUt93onUjjiL\ngEdhfqjp6IVb18OL5TAu1cUfjo+iKEkt+xIC2qqhYiVUroSqNc5EOcYN2VNh5FwomgdZk3Vi65MJ\nub8sY8x84I+AG7jdWvurA34tBUURkeD7+5s7+PmTGzhzSg5nTc35+CfIYePprKNg2bWk7nySrtg8\nKsdeTGfCyGCXJYfYW1Xw5zXQ6YdvTPbxlUkR+NS6KKEi0Ae1m53gWLUS6koBCwm5MPZ0GHMa5BwB\nLs32+zFC6kNvjHEDW4B5wG5gGXCutXbDAb2egqKISHC9ubWO8+9YwtThSXxnbjEune0dHKwldcdj\n5C+7FndfO3UFC6nLPw3r0ti1cNHQBbeugzeroDDBxS+OjmRmtv79JQR1NUP5Utj1FlSucoJkXKYT\nGMee7kyK41LL+YcIqQOyMWYGcI219sT+21cDWGuvP6DXU1AUEQmeXfUdnPrnN4mP9PCz00qIitCB\nejCIaKugcMkPSap8nY6EIirHXkJ3rFp6w9XyGvjrWqjqgEVFXn5wlI+0aLXESIjqaYfdS2HnW1Cx\nwplNNSYVRp8KE8+F3CPVPXWvA/qLyL/qqT8Akw5yLavLfnXKd/a3gzHmLGC+tfbi/ttfBKZba79x\nIH+gTouJiARJe3cfF9+9DH/AcsUJoxQSBwMbIHPzveSt+jVYS9WoL9GQOw+MQkE4m5YONx8LD26F\nh0t7eWlnL9+fHsm5Y7zqASChJyIGCo9ztt5OJyzuXAzv3g8r7oTkETD5PCc0xmcFu1oJIrUoiogE\ngbWWr963kufX7+H780czIUez0gWbr3UXI976Hgk1S2lNmUDVmIvojUoLdlkyyJS3wl/Wwpp6mJzu\n4udHR1GSqpM8MgT0djqBsfRFqF7nnCArPM4JjaNOAW9ksCsMhpA6E6SupwqKIjIE3PTSVm54YQvn\nTc9jwQSdsQ0qa8nY+k+Gr/glYNhT/AWaso5R1yv5SNbCK7vh9g3Q0gNnFHv5vyN8DItVy7MMES1V\nsO0l2PYytNdAZAKUnAVTvwzDJgS7usMppA4ExhgPzmQ2c4AKnMlsPm+tXX9Ar6egKCJyeL2woZpL\n7l7O7JGpfO3YERgFkqCJaK9gxFtXkbhnMW0p46kccwm9UanBLktCRFuv0x31iR3gMnDxhAgum+Qj\nLkKfaRkibMBZamPbi86YRn+PM/HN9Mtg9AJwD/lRbCH3YTbGnAz8AWd5jDustdcd8GspKIqIHD6b\n9rRw5i1vkREXyU9PHUeERy0QQWEt6aUPkr/8F2D9VBefR2P28WpFlANS0wH/2ASvVkBypOHyaT7O\nGe3Fq+U0ZCjpboPSF2DTU9C2B+Ky4MiLYcqXISYl2NUdKmH9IVZQFBE5TKpbulh482K6+wJce9o4\nUmJ9wS4pLEV07KHw7R+QVPkq7UljqRh3Kb1R6cEuS4aArU1wxwZn/GJBguGq6ZGckO9RrwEZWgJ+\n2L0cNv0HqlaD2wcTzoYjvzIUu6WG9YdXQVFE5DDo6Onj7L++zbbaNn6yYBwFqTHBLin89K+LWLD0\nGlz+bqqLztWMpnLQWQtLq+HOjVDeBpPSXVxxRCSzs90KjDL0NO6ETU/C9legrwuGz4LZ34WRc4ZK\nD40h8SYOlIKiiMgh5g9YLr1nOa9squGKE0YxJS8p2CWFHW9nLYXv/JDk3S/SnjiKynFfoSc6M9hl\nyRDmD8AL5fDAVqjthCMy3Xz3CB8zsob8mC4JR91tsPV5p5WxvRaGTYSjr3DWZnSF9Mk4BcVQoaAo\nIqHoZ/9Zz52Ly7hgZj4njFM4OaysJaXsSQqW/gR3Xwc1Iz9Lfd5JakWUw6bXD8+VO5Pe1HfBzCwn\nME7LVGCUIcjf67QurnsYWiohpcgJjOPPArc32NUdCAXFUKGgKCKh5q7FO7jmPxs4uSSTL87ID3Y5\nYcXTVU/hkp+QsusZOuJHUFFyGT0x2cEuS8JUtx+e2QkPlUJTN3wmx813j4hkUrrWYJQhKOCHnW/C\n2oegsQwS8mD2t2HSF0JtPUYFxVChoCgioeTFDdVces9ypuQlcfncYlyusD7eHFbJu56l8J0f4e5t\nobbwLOqGnwIu/SCX4Ovqg6fK4OFtzhqMR+e4+cZkH9PVJVWGImth9zJY+yDUboKYdKeFcdoF4AmJ\nCd3C+sCtoCgicgisq2jm7L++TVZiJD86ZSyRXoWUw8HT3UT+0mtIK3uCzrgCKkouozs2N9hlifyP\njj54ugwe3e60MB6R6eabU3wcnaNJb2QIshb2rIU1D8CeNRCfDcdeDRPPHexrMYbch9EYcwewAKix\n1pZ8qtdSUBQRObgqmzo5/ebFWGv5+eklJEZHBLuksJC4+yVGvH013u5GagsWUVtwGrgG9Q8QEbr6\n4Ply+Hcp1HXB+DQX35ziY+5wDy4FRhlqrIWqd2HV3VC3BZJHwPE/hLGLBuukNyH3ITTGfAZoA+5W\nUBQRGUQa23s4+9a3qWzq5JpTx5GbHB3skoY8d08L+ct+Tvr2f9MVm0fFuMvois8Pdlkin0ivH17a\n7Yxh3NMBo5JdfH2yj1MKPbjVbV2GGmuh/B1YdR80lUFGCcz5CRSdMNiW1RhUxQyUMSYfeFJBUURk\nkGjt6uXzf1vC5j2tfH/+KMZmJQS7pCEvseI1Ct++ioiuWuryT6O28AysWhElhPkD8FqlM0tqeRsU\nJBi+NtnHwpFevO6Q/M0q8tECfih7HVb/E1qrIOcImPszyJ8V7Mrec2AfumsS/gBMOrilsJprmr8z\nkB0PVlAclG28IiKhpqvXz8X/WM6Gqha+PadIIfEQc/W2Ufj2Dxjz8gVYl4ftR/yMmpGfVUiUkOd2\nwfE58Jdj4eqp4MJy5atdHPtAG/es76GrL3RO8It8LJcbCo+DhbfAjG9Aww6462S4/1yoKw12dWFP\nLYoiIp9Srz/AV+5ZwSubavj6cSOZNTI12CUNafFVbzHy7e8R0b6H+vxTqCk8E+vWOFAZmqyFZTXw\nr62wqRHSow2XTozg82MiiPaqhVGGmL5u2PA4rHvIWZNx2kVw7FUQnRysikLyQ6aupyIig4A/YLn8\nX6t54t1KLpxVwLyxGcEuachy9bYzfNVvyNx8D93Rw6gY9xU6E4uDXZbIYWEtrKmHB7bCmjpI9MGF\n432cPy6CxMiQ/C0r8tE6G2H1fbD1eYiIhc9cCdO/EowlNULyw3VYg6IxZj7wR8AN3G6t/dUHHvcB\ndwNTgXrgc9basv7HrgYuAvzAt6y1z/XfXwa09t/fZ62d9nF1KCiKyGBireVHj63jviW7OPeIXE6b\npMXcD5X4qsWMePsqfO2VNOTNp3rkZ7HukFiDS+Sg29AAD5bCsmqI9sB5YyO4aEIEmTEaUSRDTONO\nWHEnVCyHxOEw9xoYt+hwTngTckHRGHM/cCyQClQDP7XW/v2AXuvjgqIxxg1sAeYBu4FlwLnW2g37\n7PM1YIK19jJjzDnAImvt54wxY4H7gSOBLOBFoNha6+8PitOstXUDLVZBUUQGk18/u4lbXt3GaROz\nOPfIvGCXMyS5e1oZvvJXZGy9n+7oYVSOvYSOpNHBLktkUNjRAg+XwusVzsoCZxZ7+crECAoTtW6r\nDDGVq2D536GxzJnw5qTfQPaUw/Enh1xQPJgGEhRnANdYa0/sv301gLX2+n32ea5/n7eNMR5gD5AG\nXLXvvh/YrwwFRREJUbe8uo1fP7uJOaPTuWh2gRbIPgQSK16j8J2rieiooX74SdSMOEutiCIfoqod\nHt0Oz++CvgCcVOjhq5N8jE9TYJQhJOCHbS/BqnugswmmfBHm/BRiDum8AGF9cB9IH4VsoHyf27v7\n7/vQfay1fUAzkPIxz7XA88aYFcaYSz956SIiwXH7G9v59bObmDkihQtnKSQebO7uZka8daUzo6lx\ns+PIa6guPk8hUeQjDIuBr42HO+fC2SPhtV19nPpIO194qp23KvoIpfkoRD6Sy+2ss7jwrzD2NGcM\n401TYMlt4O8LdnVDUjDnEZ9tra0wxqQDLxhjNllrX//gTv0h8lKAvDx17RKR4Lrppa3c8MIWphck\n89VjR+DSQtgHVVL5SxQu+QHernpqC06ntmCRZjQVGaAkH3xpDJw1Ep7eCY9v9/P5JzuYmObiq5N9\nnJDvwaUTWxLqImLgiEug6ERYeis8cyWsuAtO/u1gWn9xSBhIi2IFkLvP7Zz++z50n/6upwk4k9p8\n5HOtte9d1gCP4oxj/B/W2tustdOstdPS0tIGUK6IyMFnreW3z23ihhe2cPTIVL55fBEelyaOOFi8\nnbUUvfFtRr96CQF3FNuPvJaakZ9TSBQ5ADFep2Xxjjnw9fFQ0x7gsuc7mfdgOw9t7qHHrxZGGQIS\n82DeL+DYq6G91ll/8eGLoKUy2JUNGQMZo+jBmcxmDk7IWwZ83lq7fp99vg6M32cymzOstZ81xowD\n/sneyWxeAoqASMBlrW01xsQALwDXWmuf3V8tGqMoIsFgreXaJzdw5+Iy5oxO58LZBTorf7BYS3rp\ngwxfeT2uvg7q8k+nruB0rCuYHV5EhhZ/AN6scia+2d4Cw2IMl0yM4JzRWotRhoi+Llj7MKz/N7i8\nznIaM75+MJbTCOsPyECXxzgZ+APO8hh3WGuvM8ZcCyy31j5hjIkE7gEmAw3AOdba7f3P/SFwIdAH\nfMda+4wxphCnFRGc7q//tNZe93F1KCiKyOEWCFh++Ng67l+6i/klmZx/1HCNSTxIIpu3U/jOD0io\nWUp70hgqx1xIT4yWGBE5VKyFFTXw0DZYV++sxfjlEh9fKvGSFKkeEjIEtO6BZbdD+TuQXOjMjlo0\n79O8Ylgf8AcUFAcLBUUROZz6/AG+9/AaHllVwemTsvjctFyFxIPA+LvJXn8r2Wtvxroi2FP8eZqy\njgGjH6oih8uGBqeFcUn/Woznjong4gkRDIvV51CGgIoVsPQ2aKmA4vkw/3onOH5yYX3QV1AUEfkQ\nPX0BLv/Xap5aW8XZU3M4Y0pOsEsaEuKql1L4zg+IbtlOU+ZM9hR/Eb8vIdhliYStsvfWYqx01jA/\no8jLpRMjGJmkpTUkxPl7YePjsOZfEAjArG/B7O9CRPQneRUFxVChoCgih0N7dx/f/OdKXt5cy3nT\n81gwISvYJYU8T3cjeSt/S0bpA/REpVE1+kLaUicGuywR6VfdAY9sc9Zi7AnAnOFuLpngY/owt3pS\nSGjrqIcVd8L2VyE+G068DsYudM6MfLyw/s+voCgiso+q5k4uums5m/a08OWZBcwbmxHskkJbwE9G\n6QPkrvodnt5W6vNOombEmVh3ZLArE5EP0dQNT5bB02XQ3AMlqS4umejj5AIPXndY/2aWUFe9zumO\n2rAdCj4DJ/0W0kd/3LPC+j+9gqKISL+1u5u56B/LaO3q41tzRjIpNynYJYW02JoVFCz9KbGNG2hL\nGsue0V+mO1ZdeEVCQbcfXi6Hx7bD7nbIijVcOD6Cz42OIC4irH87SygL+GHLs7D6XujthOmXwTHf\nh8j4j3pGWP9nV1AUEQGeW7+H7zywmhifm++dOJrc5E80hkH24e2sJW/lr0nf/gg9kSlUF51HS8b0\ngXbzEZFBJGBhaTU8ut2ZKTXW60x8c8H4CLI08Y2Eqq5mWHk3bH0eYtPhhF/A+LM/7DgV1gcuBUUR\nCWvWWv72xnauf3oTI9JjuWJeMYnRWuT9QJhAL5mb7ib33T9g/N3UDz+ZuoKFBDzqZioyFGxtcgLj\nm/3rmZ8ywsMlE3yMT9PENxKiajfD0luhbgvkzYCTfweZJfvuoaAYKhQUReRg6vUH+PFj63hgWTnT\nC5L52rEjifDoDPknZi2Jla8zfMV1RDeX0poykT2jzqcnZliwKxORQ6CmA57YAc/tgo4+OCrLzSUT\nIjguz4NLPQck1NiA07K48m7oaYcjL4Fjr4aoRFBQVFAUkfDT3NHL1+5bweJt9SyclM3Z03L0A+cA\nRDdsYPiK60ncs5ieqAz2FH+B1rQp6mYqEgbae+HZXU5orOuEwgQXF0+IYFGRlyivvgMkxHS3wqp7\nYPMzEJMCc34KU84P6//ICooiEnZW7Gzkm/9cSU1rN5ccXchnitOCXVLIiWivIG/1jaRufwy/N4ba\ngkU05s7FurzBLk1EDrO+gNMd9dHtUNoM8RHwudERfHFcBHnx6qUhIaa+FJbcCrUb4ZpmBcVQoaAo\nIp9GIGD56+vbuOG5LaTERvCtOUWMSIsNdlkhxd3TQva6Wxi28U7A0pA3n9r80wh4Y4JdmogEmbWw\nvgH+swPe3uNMhDNnuIfzx0UwO8etXhsSOqyFHa/BnB+H9X9aT7ALEBE5HGpbu7n8X6t5s7SOowqT\nueToQqIj9BU4UMbfQ8aW+8hZcxPeniaahs2mZsTZ9EapNVZEHMZASYqz1XXCMzvh2V19vLizj8IE\nF+eXRHBmsVfLa8jgZwwUHhvsKoJOLYoiMuS9sbWWy/+1mpbOPs6fOZzjR6VjdGZ7QEygl7Rtj5C9\n9s9EtlfQllxCddHn6YrPD3ZpIhICev3wRhU8VQabGiHGC2cWezl/XAQjkzRbqgxyBUeH9Y8FBUUR\nGbJ6/QF+/8IWbnl1G9lJUXzr+CKtjzhAHwyIHfEjqB1xFm0pEzRRjYgckC1N8OQOeL0SegMwK9vN\nl0siOD7Pg9ul7xUZhBQUFRRFZOjZWt3K9/+9hpW7mjhuVDpfmjkcn0dnrz/O/wbEQmoLz6QtdZIC\noogcFE3dztIaT+90uqjmxBq+OC6Cz472khSpyW9kEFFQVFAUkaGjq9fPza+Ucsur24j0urlgVj4z\nR6QGu6xBzwR6Sd3+KDlr/kxk+24FRBE55PwBeGcPPFkGa+rB54ZTRnj5/BgvUzPcGiIgwRfmQVEz\nOYjIkPHm1jp++NhadtZ3cPTIVM47ajgJUVquYX9cvR2kb3uYYRtu/29A3DnpSgVEETnk3C6YleVs\nZS3OOMZnt/fyyJZeihJdnDPGyxnFamUUCRa1KIpIyKtv6+YXT23k0VUVZMZHcuHsAsZnJwS7rEHN\n21FD5uZ/kLnlPjw9LXQkFFFbcDptqZMVEEUkaDr7nDGMz+2CzY0Q4YaTCjycMyaCo4aplVEOM7Uo\nioiEpkDA8tCKcn759Cbau/s4Y3I2p0/KJsKjs88fJapxM1kbbyd1x+OYgJ/W9GnUDT+FzsTiYJcm\nIkKUB07Mc7YdLfDsTnhxZx+Pl/aRH284a1QEi4q8ZMfpe17kUFOLooiEHGsti0vr+e1zm3h3dzOj\nM+O4eHYh2UlRwS5tcLKWhKo3ydpwO4lVbxBw+2jM+gwNeSfRE50Z7OpERParqw/erIIXy2FtPRhg\nRpabM0d5OanAS7Q3rBt95FAK8xZFBUURCSkrdjbw22c3886OBlJjIzhrag5HF6XhUnek/+HpbiJt\n279J33o/0S3b6Y1IpCHvBBpz5uL3xga7PBGRT6yqHV7ZDS/vhqoOiPbASYVezhrlZfowt44FcnAp\nKCooisjgt76ymRue28zLm2tJiPKycFIWc8Zk4HWr+9H7WEtczTIytj5Ays6ncQV66EgoojHneJoz\nZ2JdmtxHREKftbC+AV7aDW9WQkcfZMYYTin0cupILxPTXBrPKJ+egqKCoogMXqU1bfz+hS08tbaK\nWJ+HBROGceK4TCK9WhNxX57uJlK3P0LGlvuJbtmG3xNNc+YsGnKOpztueLDLExE5ZLr6nGU2Xq+E\nFTXQZyEnzrBghJcFI7yMS1FolAOkoKigKCKDi7WWt7bVc9dbZby0sZoIj4uTSoZxyvhhxPg0B9d7\nTKCXhKrFpO54nJRdz+Lyd9MRP4LGnDk0Zx6FdUcGu0QRkcOqrRfe3gNvVMKqWghYyI93QuMJBV5K\nUl3qnioDp6CooCgig0N7dx+PrKrgH2+VUVrTRnykh+NGp3NSyTCth/gea4mtXUla2ROklD2Ft7sB\nvyeG5syjaMyZQ1dcfrArFBEZFJq74a3+0Li2DgJAerRhTp6HOcM9zMr2EKWJcGR/FBQVFEUkuMrq\n2rn77Z08tKKc1q4+ClJjOHFcBjMKU7XURb+opi2k7niC1B2PE9leQcAVQWvaZJozZ9GWOlFjD0VE\n9qO5G5bXwJJqp6Wxow8i3TA7xwmNx+V5yIzR8UY+IMyDovpwiUhQtHb18vz6ah5fXcEbW+twuQxH\nFiQzf1wmRemxGk9iLdGNG0guf5HkXc8R07QJi4u2lBLq8k+lNX0aAU90sKsUEQkJCT6Yk+tsvQFY\nV++ExqV7+nhxZx8ABQmGmdkeZmR5OCrLTWqUgqOEN7Uoishh09nj5+VNNfzn3Upe3lRDjz9AepyP\n2UWpzBmdQXJMRLBLDCrj7yG+eglJu18iufxFfB2VWAwdiUW0ZMygOeMo/L6EYJcpIjJkWAs7W51W\nxnfrnJlUO5zcSHGSqz84upmS4SYtWsEx7IR5i6KCoogcUl29fhaX1vGfdyt5fkM1HT1+EqO9HFWQ\nwswRKYwM89ZDd3cziZWvkbz7RRIrXsXT20bAFUFbynha06bSmjpZ4VBE5DDxB2BrM6ypc4Ljhgbo\nCTiPZcUaJqa5mZjuZkKam/FpbuIiwvf4FRbCPCiq66mIHFTWWsrqO3htcw2vbanl7e31dPUGiPV5\nOKowhRmFKYwdFo/LFZ7fvcbfTVzNchL2vEVi1WJiGtZhbIC+iHha06bRmjaVtpQSrNsX7FJFRMKO\n2wWjk5zts0XQ64fNTbClCbY2WVZV9/HMDqfJ0QCFiS4mpLkZmehiRJKLkYkuhse78LrD8xgnQ4uC\nooh8am3dfby9rZ7XttTw6uZadjd2ApAZH8lnitKYlJvI+OwEPO4w7LYT8BPTuIGEqsUkVC0mvnY5\nLn831rjoSCiitmAhbSkT6EwYCSYM/35ERAYxrxtKUpztPc3dTqujEx4DvLE7wKNb9z7uMTA8wQmN\nI5Nc5MW7yIp1MSzGkBXrIlozrUqIUFAUkU+sqrmT5WWNrNjZyPKdDWysbMVvLZFeF2OHJTBvbAYT\ncxLJiA+/dfxcvW3E1r1LXO0q4mpXEFe7Ck9vCwBdsbk0Zh9PW3IJHUmjCXiiglytiIh8Ugk+mJbu\nbO/p6IOKNihvhfI2KG8LsKEuwIs7wW//9/lZsS6yY11kxhhSogzJkS6SIg3JkeZ9l5EehUoJHgVF\nEdmvrl4/m/a08m55E8t3NrKirIHK5i4AIjwuRqbFcurEYYzLSmBUZhzecGo1tBZf2659QuEKopu2\nYGwAi6E7NpvW9Km0J42lPXkcfb7EYFcsIiKHQLQHihKdbV99AajvgtrO9291XQG2NQZYUgWtPR/9\nuj43xHoNMREQ5zXERBjivIbYCIjxOkEy0g0+j8Hnhsj+y/9v795j5DrLO45/n3NmzlzW60tsx9hO\niA3YSIFKEC6hLVBKSQhQ4dCWEoTKLWogCrSoarm0FY2QqnIprYCiIlrSBkQIUEproZZwq6CiDeRC\ngNgEsJ3QOBjb8WXvM3MuT/84765n7V1n49h7xru/j3T0vuedd47fmdfv7DznvOed/nwzNho1aMRG\nM6SNGJIYkthIIqhFLOv1AmRuChRFZMaR8S67D4yy++ej7D4wyq6fj7Lv8DhFOBt6wVDCtgtX8KJL\nN7B9wzCXrG1Ti5ZHYGhFSmtkL+2juxk6tpuho7sYOvqjmauFea3F1KoncXjr1Uyu2sbUqidR1Icq\nbrWIiFSpFsGGdrnNJy9gPIWRHoz2+tIuTGQwlTmT02kXjkwS9qGXQzc/9arlo2VMB45Qj0IAGUMS\n2Ux5Mh1g9pXVZ/Ings/6Sc9JZh2vLz/rWCfKWjWjVYNIgWvlFCiKLDPuzoGRDvsOT7D38Hi5HRrn\np4fGOTTWnam3bkXC4y8Y4uqnbWbL2iG2rBti3Ypk6Z9x9IJk8gCtkX1lYHj8PoaO7qZ9/CdERXna\nt4gSOisuZvTCZ9FZuYXJVdvorrhI9xiKiMijFkfldNRVj2ENs7wofx+ym5dpryiDyJk05Kfr9EK9\nLJSdvGWFh3xIc5jKYfSUen374Zhn6/cU2jVo1412HYZq5dXUoTqsTIyVDWNlYqwK6cpGmV/dKKfy\nrm0ZDS0o9JgpUBRZgjppzs+PT/HgsSn2H5tk/7EpHjw6yf0PT7Dv4QmmevlM3XYSs3FVkydvGOaK\nSzewZe0Ql6xtM9ysV/gKzr0oHac5/iDN0ftpj+yhNbKX5sheWqP7iPPOTL2sPkxn+BKOXnwFneEt\ndIYvodveCFFcYetFREROiKNya1b8zd69vLo5VwB6SlkO6XTd/ER5NwSlnXAVtZOX6VQKIx3Yk5ZX\nYMfT019JHU5gbdNY24pYF4LHdS1jXSuaCSany1c1TFcw56BAUeQ84u6MdzMOjnY5NNrh0FiXg6Md\nDo52OTjW4RcjHf7v6CSH+64MAtQiY92KBheubPBr29ezaVWLTaubbFrdYnWrviSvEkbpBI3JAzTG\n99MYf5Dm+P6ZfGN8P/Xe8Zm6jpG21tNtb+T45l+nO7SJbnsT3aFN5MlKWILvj4iIyNlmVq76Wovg\nXC9n514GleNpOUV3vFtJFdgAAA2KSURBVAejaTll93i3nMJ7vOuM9HLuO1KWjfbmvuIZG6xtGetb\nxvp2xPp2mX/71nP8IgacuZ+tC8Tn3sbNm/0N118/sx9HRhRF1GKjHkfUIiOOY4jqxPU6tXqDZqNB\nkiQkjQbNRpNm0qDRbNJqNmm3whbyQ60WcVzTl0I559ydTlow1kkZ62aMdTLGOxljnZTjUylHJ3oc\nnehxbKLHkZA/MtHl2ETKVJqfcrxmPeKCdsLqdsL64QbrhxtcONxg/Yoyv6adLI3fLSwyat3j1LtH\nqXePUZ86TDJ5kGTqEPWQJlMHSSYPEWcTs58a1em11pM215G21pM219NrXUh3aCO99uP0u4UiIiJL\nXO7l4kEnAskQWPbgWBeOdcqyY2Hb+1cvWwJfns7cggJFM7sK+BAQA//o7u896fEG8EngGcAR4FXu\n/kB47F3AtUAO/IG737aQY87lmZtiv/O6FQt+cWeqR42UOpnVyaKE3BKKOMGjBK8lWNzAag2s3iSq\nN4nrDWpJk3rSpN5oEtebWL0JcQK1Rl/agFoS0sYcZdNp80Q+ritwrVhROJ0sZ6qXM5XmdNKcqV7B\nVFruT/VCWXqizkQI/sY6KePdjNGQL8syJroZWXH6sdesRww36ww3aww3aqwM+dXthDVDCWvadda0\nE9a0E1rJeTIN0p0o7xClE8TZBLXeGHFvlFo6RpyOEffGynxvjFpvpAwKO0eohcCw1hud87BFlJA2\n1pA1VpM11pA11pA2VpM1LiiDw9Z6smSV7iEUERGRBSkcfum5yztQfMSpp2YWAx8FrgD2A3eY2U53\n391X7VrgmLs/ycyuAd4HvMrMLgWuAZ4CbAK+Zmbbw3Me6Zin6K64iD3Peefp2+sF5hl4jhU55jkU\nGXmek2ZhyzOyLCfPc7I8J8/Lx4vprciwPIUixUI+ylKiIqPmKQ2bIGGEhJSErEytTBukYf/Uqz5n\nKo8SPASqRQguPT45uGxgtQSPm3MEniFY7QterdbA4gbEMViERTEW1TCLIKqF/RiLIrC4vB/LYtyi\nsMU4hkdxmbeynhPjkZWpRRTT9R1www3cjSK8tsKhoLwJO8+d1J08zGMv04Lcy5uu+/ezojwrlOYF\neeFkhZ9I86K8CTt3ellBL8/LNCvoTqd5QTct6OUFvSyfyc88nuV00oJOmtPNitP0ztziyGgnMe16\nRDup0apHDCUx69qtsjyJadUjWkmtzCcR7VpMK4kZasSsbNZJaiGomXUyxzH38v+3F+A9rJiCqaL8\nv08R/u8XQIHN1CvzuM9RVoT9PIyfAitSrOgR5b1T0qjoYXmPqEhDOr3fnbUfZ1NE2SRxNtmXdrAF\n3Oaex02KWpssGSavr6DX3sjk6u3k9ZXkyTBZvSyfDgiL2pBOqIiIiMhZsxQmYj1WC7lH8dnAHnff\nB2BmtwI7gP6gbgdwY8j/C/B3Vt70tAO41d27wP1mticcjwUcc47WtvDHPfW0VU73FTQO22OZM50X\n5RLF4z1nIj15g4me082dblqQ5Sl5mpLnKZ71KPIUz1M8K1NCMBoXvVMCzoSsDDotpTH92HRgatMB\naUZCl8QmZvYb9GbqzByTjPpZClwtbIOm8DL88NA6Z/Y+2EzDTw5UbFbqfeUeJtrP89gc+VOkYVti\n3GI8qpdbXD+Rj2ozaRE38dYaevFGvNakiJsUtSYeNylqDYpam6I+NJPm9aGZ/UezUIxutBYRERE5\n+xbyHWsz8GDf/n7g8vnquHtmZiPA2lB++0nP3Rzyj3RMAMzsOuC6sNvd/vTn37uANsviWAc8XHUj\nZIb6Y/CoTwaL+mPwqE8Gi/pjsKg/qvdld7+q6kZUZeBPxrv7x4GPA5jZne7+zIqbJIH6Y7CoPwaP\n+mSwqD8Gj/pksKg/Bov6Q6q2kJUdHgIu7tu/KJTNWcfMasAqykVt5nvuQo4pIiIiIiIiFVhIoHgH\nsM3MtppZQrk4zc6T6uwEXhfyvwN8w8vlVHcC15hZw8y2AtuA7y7wmCIiIiIiIlKBR5x6Gu45fAtw\nG+VaMDe5+y4zew9wp7vvBD4BfCosVnOUMvAj1Psc5SI1GXCDu+cAcx1zAe39+KN+hXIuqT8Gi/pj\n8KhPBov6Y/CoTwaL+mOwqD+kUgv6HUURERERERFZPvTr0yIiIiIiIjKLAkURERERERGZ5bwIFM3s\nKjP7sZntMbN3Vt2e5cbMLjaz/zKz3Wa2y8z+MJTfaGYPmdk9YXtp1W1dTszsATP7YXjv7wxlF5jZ\nV83spyFdU3U7lwMze3LfOLjHzEbN7G0aI4vLzG4ys0Nmdm9f2ZxjwkofDn9XfmBml1XX8qVpnv74\ngJndF97zL5rZ6lC+xcym+sbKx6pr+dI1T5/M+zllZu8KY+THZvbialq9dM3TH5/t64sHzOyeUK4x\nIotu4O9RNLMY+AlwBbCfcsXUV7v77kobtoyY2UZgo7vfbWbDwF3A1cDvAuPu/teVNnCZMrMHgGe6\n+8N9Ze8Hjrr7e8NJlTXu/o6q2rgchc+sh4DLgTegMbJozOz5wDjwSXd/aiibc0yEL8NvBV5K2Vcf\ncvfLq2r7UjRPf1xJuTJ6ZmbvAwj9sQX40nQ9OTfm6ZMbmeNzyswuBT4DPBvYBHwN2D69KKE8dnP1\nx0mPfxAYcff3aIxIFc6HK4rPBva4+z537wG3AjsqbtOy4u4H3P3ukB8DfgRsrrZVMo8dwM0hfzNl\nQC+L6zeAve7+s6obsty4+7coV97uN9+Y2EH55czd/XZgdTgpJmfJXP3h7l9x9yzs3k75O8qySOYZ\nI/PZAdzq7l13vx/YQ/mdTM6S0/WHmRnlCfnPLGqjRPqcD4HiZuDBvv39KEipTDij9XTgO6HoLWEK\n0U2a5rjoHPiKmd1lZteFsg3ufiDkfwFsqKZpy9o1zP7DrjFSrfnGhP62VO+NwH/27W81s++Z2TfN\n7HlVNWqZmutzSmOkWs8DDrr7T/vKNEZkUZ0PgaIMCDNbAXwBeJu7jwJ/DzwReBpwAPhghc1bjp7r\n7pcBLwFuCFNYZng5r3yw55YvMWaWAC8HPh+KNEYGiMbE4DCzP6P8feVPh6IDwOPd/enAHwG3mNnK\nqtq3zOhzajC9mtknHTVGZNGdD4HiQ8DFffsXhTJZRGZWpwwSP+3u/wrg7gfdPXf3AvgHNCVlUbn7\nQyE9BHyR8v0/OD19LqSHqmvhsvQS4G53PwgaIwNivjGhvy0VMbPXA78JvCYE74TpjUdC/i5gL7C9\nskYuI6f5nNIYqYiZ1YDfAj47XaYxIlU4HwLFO4BtZrY1nK2/BthZcZuWlTBP/hPAj9z9b/rK++/n\neQVw78nPlXPDzIbCwkKY2RBwJeX7vxN4Xaj2OuDfq2nhsjXrDLDGyECYb0zsBF4bVj99DuWCEQfm\nOoCcPWZ2FfB24OXuPtlXvj4sBIWZPQHYBuyrppXLy2k+p3YC15hZw8y2UvbJdxe7fcvUi4D73H3/\ndIHGiFShVnUDHklYGe0twG1ADNzk7rsqbtZy86vA7wE/nF6mGfhT4NVm9jTKqVwPAG+qpnnL0gbg\ni2UMTw24xd2/bGZ3AJ8zs2uBn1HeCC+LIATsVzB7HLxfY2TxmNlngBcA68xsP/AXwHuZe0z8B+WK\np3uAScoVauUsmqc/3gU0gK+Gz6/b3f3NwPOB95hZChTAm919oYuuyALN0ycvmOtzyt13mdnngN2U\n04Rv0IqnZ9dc/eHun+DUe91BY0QqMPA/jyEiIiIiIiKL63yYeioiIiIiIiKLSIGiiIiIiIiIzKJA\nUURERERERGZRoCgiIiIiIiKzKFAUERERERGRWRQoiojIQDKzDWZ2i5ntM7O7zOx/zewVZvYCM/tS\n1e0TERFZyhQoiojIwLHyR/b+DfiWuz/B3Z9B+dtiF1XbMhERkeVBgaKIiAyiFwI9d//YdIG7/8zd\nP9JfycxuNLM/7tu/18y2hPxrzewHZvZ9M/tUKNtiZt8I5V83s8eH8leG537fzL4VymIz+4CZ3RHq\nv+mcv2oREZEBUau6ASIiInN4CnD3mT7ZzJ4C/DnwK+7+sJldEB76CHCzu99sZm8EPgxcDbwbeLG7\nP2Rmq0Pda4ERd3+WmTWAb5vZV9z9/jNtl4iIyPlCVxRFRGTgmdlHw9W+Oxb4lBcCn3f3hwHc/Wgo\n/2XglpD/FPDckP828M9m9vtAHMquBF5rZvcA3wHWAtse2ysRERE5P+iKooiIDKJdwG9P77j7DWa2\nDrjzpHoZs096Ns/kH3P3N5vZ5cDLgLvM7BmAAW9199vO5JgiIiLnM11RFBGRQfQNoGlm1/eVteeo\n9wBwGYCZXQZs7Xv+K81sbXhseurp/1AuigPwGuC/w+NPdPfvuPu7gcPAxcBtwPVmVg91tpvZ0Nl5\neSIiIoNNVxRFRGTguLub2dXA35rZ2ymDtwngHSdV/QLl9NBdlNNDfxKev8vM/hL4ppnlwPeA1wNv\nBf7JzP4kHPMN4TgfMLNtlFcRvw58H/gBsAW4O6zCepjyfkYREZElz9y96jaIiIiIiIjIANHUUxER\nEREREZlFgaKIiIiIiIjMokBRREREREREZlGgKCIiIiIiIrMoUBQREREREZFZFCiKiIiIiIjILAoU\nRUREREREZJb/BwLeJJG8KcHNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 917.25x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeuhD1auVTvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize = (10,8))\n",
        "\n",
        "plotting.andrews_curves(df.drop(\"Age\", axis=1), \"Outcome\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9QRXx1hXRBt",
        "colab_type": "code",
        "outputId": "b5fcd15c-521a-4d25-872c-6eec39292621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            "Pregnancies                 768 non-null int64\n",
            "Glucose                     768 non-null int64\n",
            "BloodPressure               768 non-null int64\n",
            "SkinThickness               768 non-null int64\n",
            "Insulin                     768 non-null int64\n",
            "BMI                         768 non-null float64\n",
            "DiabetesPedigreeFunction    768 non-null float64\n",
            "Age                         768 non-null int64\n",
            "Outcome                     768 non-null int64\n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K10jQBWYXM3a",
        "colab_type": "text"
      },
      "source": [
        "# **set data of train & test**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U16ujMPBXNN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = data[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]\n",
        "y = data['Outcome']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ66DxYSfAea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyzywUZGXZE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4MwSoJzVs7Z",
        "colab_type": "text"
      },
      "source": [
        "# **train Dataset (only apllyed train data set)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_HjGSQaWIPn",
        "colab_type": "code",
        "outputId": "1029e661-8118-4d54-8f15-74f9c6cc739e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "# test_size = 0.3 means 30% , 0.5 meas 50%\n",
        "train, test = train_test_split(data, test_size = 0.3)# in this our main data is split into train and test\n",
        "# the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(537, 9)\n",
            "(231, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWMH9aSZi0V5",
        "colab_type": "code",
        "outputId": "93c30b7a-d212-470d-ee8f-bf14dc522c33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# নতুন  y অবজেক্টগুলোর রেকর্ড সংখ্যা \n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f4478d8331e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-9eZuoUZTe0",
        "colab_type": "code",
        "outputId": "d38ed2f4-2db0-4428-ca25-fb634afed477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxIQXnD-d_sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke0Fi2n4H9cN",
        "colab_type": "code",
        "outputId": "2ce73424-e375-43b8-924b-32a4f494f070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# নতুন  y অবজেক্টগুলোর রেকর্ড সংখ্যা \n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(537,)\n",
            "(231,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOEKN_YNfIrz",
        "colab_type": "code",
        "outputId": "73303fa8-0da8-4dd0-801b-2da9d1152306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# ৫টা রেকর্ডের মধ্যে ৩টা এসেছে এখানে \n",
        "x_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>711</th>\n",
              "      <td>5</td>\n",
              "      <td>126</td>\n",
              "      <td>78</td>\n",
              "      <td>27</td>\n",
              "      <td>22</td>\n",
              "      <td>29.6</td>\n",
              "      <td>0.439</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0</td>\n",
              "      <td>146</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40.5</td>\n",
              "      <td>1.781</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>5</td>\n",
              "      <td>109</td>\n",
              "      <td>62</td>\n",
              "      <td>41</td>\n",
              "      <td>129</td>\n",
              "      <td>35.8</td>\n",
              "      <td>0.514</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>4</td>\n",
              "      <td>110</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31.9</td>\n",
              "      <td>0.471</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>11</td>\n",
              "      <td>127</td>\n",
              "      <td>106</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.190</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>2</td>\n",
              "      <td>114</td>\n",
              "      <td>68</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>28.7</td>\n",
              "      <td>0.092</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>4</td>\n",
              "      <td>111</td>\n",
              "      <td>72</td>\n",
              "      <td>47</td>\n",
              "      <td>207</td>\n",
              "      <td>37.1</td>\n",
              "      <td>1.390</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>0</td>\n",
              "      <td>165</td>\n",
              "      <td>90</td>\n",
              "      <td>33</td>\n",
              "      <td>680</td>\n",
              "      <td>52.3</td>\n",
              "      <td>0.427</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>637</th>\n",
              "      <td>2</td>\n",
              "      <td>94</td>\n",
              "      <td>76</td>\n",
              "      <td>18</td>\n",
              "      <td>66</td>\n",
              "      <td>31.6</td>\n",
              "      <td>0.649</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>668</th>\n",
              "      <td>6</td>\n",
              "      <td>98</td>\n",
              "      <td>58</td>\n",
              "      <td>33</td>\n",
              "      <td>190</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.430</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>1</td>\n",
              "      <td>81</td>\n",
              "      <td>74</td>\n",
              "      <td>41</td>\n",
              "      <td>57</td>\n",
              "      <td>46.3</td>\n",
              "      <td>1.096</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>617</th>\n",
              "      <td>2</td>\n",
              "      <td>68</td>\n",
              "      <td>62</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "      <td>20.1</td>\n",
              "      <td>0.257</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>13</td>\n",
              "      <td>106</td>\n",
              "      <td>72</td>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>36.6</td>\n",
              "      <td>0.178</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>8</td>\n",
              "      <td>112</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.6</td>\n",
              "      <td>0.840</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>7</td>\n",
              "      <td>194</td>\n",
              "      <td>68</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>35.9</td>\n",
              "      <td>0.745</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>10</td>\n",
              "      <td>148</td>\n",
              "      <td>84</td>\n",
              "      <td>48</td>\n",
              "      <td>237</td>\n",
              "      <td>37.6</td>\n",
              "      <td>1.001</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>6</td>\n",
              "      <td>107</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.727</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>62</td>\n",
              "      <td>24</td>\n",
              "      <td>44</td>\n",
              "      <td>29.9</td>\n",
              "      <td>0.422</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>122</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22.4</td>\n",
              "      <td>0.207</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>1</td>\n",
              "      <td>73</td>\n",
              "      <td>50</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.248</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>0</td>\n",
              "      <td>107</td>\n",
              "      <td>62</td>\n",
              "      <td>30</td>\n",
              "      <td>74</td>\n",
              "      <td>36.6</td>\n",
              "      <td>0.757</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>7</td>\n",
              "      <td>179</td>\n",
              "      <td>95</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>34.2</td>\n",
              "      <td>0.164</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>2</td>\n",
              "      <td>146</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27.5</td>\n",
              "      <td>0.240</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>5</td>\n",
              "      <td>85</td>\n",
              "      <td>74</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1.224</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0</td>\n",
              "      <td>101</td>\n",
              "      <td>65</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>24.6</td>\n",
              "      <td>0.237</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>6</td>\n",
              "      <td>85</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31.2</td>\n",
              "      <td>0.382</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>8</td>\n",
              "      <td>176</td>\n",
              "      <td>90</td>\n",
              "      <td>34</td>\n",
              "      <td>300</td>\n",
              "      <td>33.7</td>\n",
              "      <td>0.467</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>5</td>\n",
              "      <td>78</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33.7</td>\n",
              "      <td>0.654</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>8</td>\n",
              "      <td>196</td>\n",
              "      <td>76</td>\n",
              "      <td>29</td>\n",
              "      <td>280</td>\n",
              "      <td>37.5</td>\n",
              "      <td>0.605</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>0</td>\n",
              "      <td>113</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33.3</td>\n",
              "      <td>0.278</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>735</th>\n",
              "      <td>4</td>\n",
              "      <td>95</td>\n",
              "      <td>60</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>35.4</td>\n",
              "      <td>0.284</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>7</td>\n",
              "      <td>62</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32.6</td>\n",
              "      <td>0.391</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638</th>\n",
              "      <td>7</td>\n",
              "      <td>97</td>\n",
              "      <td>76</td>\n",
              "      <td>32</td>\n",
              "      <td>91</td>\n",
              "      <td>40.9</td>\n",
              "      <td>0.871</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>70</td>\n",
              "      <td>52</td>\n",
              "      <td>57</td>\n",
              "      <td>40.5</td>\n",
              "      <td>0.677</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>1</td>\n",
              "      <td>111</td>\n",
              "      <td>86</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.143</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556</th>\n",
              "      <td>1</td>\n",
              "      <td>97</td>\n",
              "      <td>70</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>38.1</td>\n",
              "      <td>0.218</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>1</td>\n",
              "      <td>83</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18.2</td>\n",
              "      <td>0.624</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0</td>\n",
              "      <td>105</td>\n",
              "      <td>64</td>\n",
              "      <td>41</td>\n",
              "      <td>142</td>\n",
              "      <td>41.5</td>\n",
              "      <td>0.173</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>0</td>\n",
              "      <td>118</td>\n",
              "      <td>64</td>\n",
              "      <td>23</td>\n",
              "      <td>89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.731</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>70</td>\n",
              "      <td>26</td>\n",
              "      <td>50</td>\n",
              "      <td>30.8</td>\n",
              "      <td>0.597</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>0</td>\n",
              "      <td>180</td>\n",
              "      <td>90</td>\n",
              "      <td>26</td>\n",
              "      <td>90</td>\n",
              "      <td>36.5</td>\n",
              "      <td>0.314</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>623</th>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>115</td>\n",
              "      <td>43.5</td>\n",
              "      <td>0.347</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>7</td>\n",
              "      <td>105</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.305</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>3</td>\n",
              "      <td>180</td>\n",
              "      <td>64</td>\n",
              "      <td>25</td>\n",
              "      <td>70</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.271</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>7</td>\n",
              "      <td>114</td>\n",
              "      <td>76</td>\n",
              "      <td>17</td>\n",
              "      <td>110</td>\n",
              "      <td>23.8</td>\n",
              "      <td>0.466</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>116</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25.6</td>\n",
              "      <td>0.201</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>3</td>\n",
              "      <td>171</td>\n",
              "      <td>72</td>\n",
              "      <td>33</td>\n",
              "      <td>135</td>\n",
              "      <td>33.3</td>\n",
              "      <td>0.199</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>704</th>\n",
              "      <td>4</td>\n",
              "      <td>110</td>\n",
              "      <td>76</td>\n",
              "      <td>20</td>\n",
              "      <td>100</td>\n",
              "      <td>28.4</td>\n",
              "      <td>0.118</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>732</th>\n",
              "      <td>2</td>\n",
              "      <td>174</td>\n",
              "      <td>88</td>\n",
              "      <td>37</td>\n",
              "      <td>120</td>\n",
              "      <td>44.5</td>\n",
              "      <td>0.646</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>3</td>\n",
              "      <td>99</td>\n",
              "      <td>80</td>\n",
              "      <td>11</td>\n",
              "      <td>64</td>\n",
              "      <td>19.3</td>\n",
              "      <td>0.284</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757</th>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36.3</td>\n",
              "      <td>0.258</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>644</th>\n",
              "      <td>3</td>\n",
              "      <td>103</td>\n",
              "      <td>72</td>\n",
              "      <td>30</td>\n",
              "      <td>152</td>\n",
              "      <td>27.6</td>\n",
              "      <td>0.730</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>6</td>\n",
              "      <td>114</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27.8</td>\n",
              "      <td>0.247</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>4</td>\n",
              "      <td>116</td>\n",
              "      <td>72</td>\n",
              "      <td>12</td>\n",
              "      <td>87</td>\n",
              "      <td>22.1</td>\n",
              "      <td>0.463</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>24.7</td>\n",
              "      <td>0.140</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>1</td>\n",
              "      <td>109</td>\n",
              "      <td>38</td>\n",
              "      <td>18</td>\n",
              "      <td>120</td>\n",
              "      <td>23.1</td>\n",
              "      <td>0.407</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575</th>\n",
              "      <td>1</td>\n",
              "      <td>119</td>\n",
              "      <td>44</td>\n",
              "      <td>47</td>\n",
              "      <td>63</td>\n",
              "      <td>35.5</td>\n",
              "      <td>0.280</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>5</td>\n",
              "      <td>115</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31.2</td>\n",
              "      <td>0.343</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>9</td>\n",
              "      <td>130</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>34.2</td>\n",
              "      <td>0.652</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>537 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  BloodPressure  ...   BMI  DiabetesPedigreeFunction  Age\n",
              "711            5      126             78  ...  29.6                     0.439   40\n",
              "58             0      146             82  ...  40.5                     1.781   44\n",
              "216            5      109             62  ...  35.8                     0.514   25\n",
              "168            4      110             66  ...  31.9                     0.471   29\n",
              "658           11      127            106  ...  39.0                     0.190   51\n",
              "257            2      114             68  ...  28.7                     0.092   25\n",
              "39             4      111             72  ...  37.1                     1.390   56\n",
              "247            0      165             90  ...  52.3                     0.427   23\n",
              "637            2       94             76  ...  31.6                     0.649   23\n",
              "668            6       98             58  ...  34.0                     0.430   43\n",
              "747            1       81             74  ...  46.3                     1.096   32\n",
              "617            2       68             62  ...  20.1                     0.257   23\n",
              "86            13      106             72  ...  36.6                     0.178   45\n",
              "299            8      112             72  ...  23.6                     0.840   58\n",
              "185            7      194             68  ...  35.9                     0.745   41\n",
              "458           10      148             84  ...  37.6                     1.001   51\n",
              "439            6      107             88  ...  36.8                     0.727   31\n",
              "553            1       88             62  ...  29.9                     0.422   23\n",
              "106            1       96            122  ...  22.4                     0.207   27\n",
              "55             1       73             50  ...  23.0                     0.248   21\n",
              "291            0      107             62  ...  36.6                     0.757   25\n",
              "212            7      179             95  ...  34.2                     0.164   60\n",
              "269            2      146              0  ...  27.5                     0.240   28\n",
              "218            5       85             74  ...  29.0                     1.224   32\n",
              "83             0      101             65  ...  24.6                     0.237   22\n",
              "176            6       85             78  ...  31.2                     0.382   42\n",
              "53             8      176             90  ...  33.7                     0.467   58\n",
              "117            5       78             48  ...  33.7                     0.654   25\n",
              "206            8      196             76  ...  37.5                     0.605   57\n",
              "124            0      113             76  ...  33.3                     0.278   23\n",
              "..           ...      ...            ...  ...   ...                       ...  ...\n",
              "735            4       95             60  ...  35.4                     0.284   28\n",
              "76             7       62             78  ...  32.6                     0.391   41\n",
              "638            7       97             76  ...  40.9                     0.871   32\n",
              "275            2      100             70  ...  40.5                     0.677   25\n",
              "249            1      111             86  ...  30.1                     0.143   23\n",
              "556            1       97             70  ...  38.1                     0.218   30\n",
              "418            1       83             68  ...  18.2                     0.624   27\n",
              "59             0      105             64  ...  41.5                     0.173   22\n",
              "371            0      118             64  ...   0.0                     1.731   21\n",
              "136            0      100             70  ...  30.8                     0.597   21\n",
              "506            0      180             90  ...  36.5                     0.314   35\n",
              "623            0       94             70  ...  43.5                     0.347   21\n",
              "49             7      105              0  ...   0.0                     0.305   24\n",
              "40             3      180             64  ...  34.0                     0.271   26\n",
              "477            7      114             76  ...  23.8                     0.466   31\n",
              "5              5      116             74  ...  25.6                     0.201   30\n",
              "110            3      171             72  ...  33.3                     0.199   24\n",
              "704            4      110             76  ...  28.4                     0.118   27\n",
              "732            2      174             88  ...  44.5                     0.646   24\n",
              "316            3       99             80  ...  19.3                     0.284   30\n",
              "757            0      123             72  ...  36.3                     0.258   52\n",
              "765            5      121             72  ...  26.2                     0.245   30\n",
              "644            3      103             72  ...  27.6                     0.730   27\n",
              "552            6      114             88  ...  27.8                     0.247   66\n",
              "393            4      116             72  ...  22.1                     0.463   37\n",
              "75             1        0             48  ...  24.7                     0.140   22\n",
              "599            1      109             38  ...  23.1                     0.407   26\n",
              "575            1      119             44  ...  35.5                     0.280   25\n",
              "337            5      115             76  ...  31.2                     0.343   44\n",
              "523            9      130             70  ...  34.2                     0.652   45\n",
              "\n",
              "[537 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-YsanBzfPIB",
        "colab_type": "code",
        "outputId": "404e4d15-41cd-43aa-f3f2-79b846059e6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "# টার্গেট ভেক্টর আসতে হবে ওই ৩টাই \n",
        "y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
              "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
              "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL6LfiSCgf19",
        "colab_type": "code",
        "outputId": "99f1cd3a-dc5a-4258-970a-340a4878d2f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_test\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "766    1\n",
              "748    1\n",
              "42     0\n",
              "485    1\n",
              "543    0\n",
              "445    1\n",
              "427    1\n",
              "352    0\n",
              "428    0\n",
              "123    0\n",
              "525    0\n",
              "587    0\n",
              "616    0\n",
              "648    1\n",
              "678    1\n",
              "227    1\n",
              "31     1\n",
              "113    0\n",
              "116    1\n",
              "210    0\n",
              "318    0\n",
              "692    0\n",
              "689    1\n",
              "683    1\n",
              "423    0\n",
              "715    1\n",
              "446    0\n",
              "267    0\n",
              "90     0\n",
              "191    0\n",
              "      ..\n",
              "346    0\n",
              "746    1\n",
              "109    1\n",
              "347    0\n",
              "72     1\n",
              "336    0\n",
              "545    1\n",
              "548    0\n",
              "464    0\n",
              "400    1\n",
              "16     1\n",
              "601    0\n",
              "80     0\n",
              "98     0\n",
              "126    0\n",
              "182    0\n",
              "279    0\n",
              "752    0\n",
              "62     0\n",
              "172    0\n",
              "213    1\n",
              "111    1\n",
              "384    0\n",
              "268    0\n",
              "186    1\n",
              "188    1\n",
              "8      1\n",
              "645    0\n",
              "381    0\n",
              "314    1\n",
              "Name: Outcome, Length: 231, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcFyGwEKf93K",
        "colab_type": "code",
        "outputId": "97a322c6-e4d6-4e20-fd85-336ee99f237b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>748</th>\n",
              "      <td>3</td>\n",
              "      <td>187</td>\n",
              "      <td>70</td>\n",
              "      <td>22</td>\n",
              "      <td>200</td>\n",
              "      <td>36.4</td>\n",
              "      <td>0.408</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>7</td>\n",
              "      <td>106</td>\n",
              "      <td>92</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>22.7</td>\n",
              "      <td>0.235</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>0</td>\n",
              "      <td>135</td>\n",
              "      <td>68</td>\n",
              "      <td>42</td>\n",
              "      <td>250</td>\n",
              "      <td>42.3</td>\n",
              "      <td>0.365</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>4</td>\n",
              "      <td>84</td>\n",
              "      <td>90</td>\n",
              "      <td>23</td>\n",
              "      <td>56</td>\n",
              "      <td>39.5</td>\n",
              "      <td>0.159</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>0</td>\n",
              "      <td>180</td>\n",
              "      <td>78</td>\n",
              "      <td>63</td>\n",
              "      <td>14</td>\n",
              "      <td>59.4</td>\n",
              "      <td>2.420</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>1</td>\n",
              "      <td>181</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>180</td>\n",
              "      <td>34.1</td>\n",
              "      <td>0.328</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>3</td>\n",
              "      <td>61</td>\n",
              "      <td>82</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>34.4</td>\n",
              "      <td>0.243</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>0</td>\n",
              "      <td>135</td>\n",
              "      <td>94</td>\n",
              "      <td>46</td>\n",
              "      <td>145</td>\n",
              "      <td>40.6</td>\n",
              "      <td>0.284</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>5</td>\n",
              "      <td>132</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.8</td>\n",
              "      <td>0.186</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>3</td>\n",
              "      <td>87</td>\n",
              "      <td>60</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>21.8</td>\n",
              "      <td>0.444</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>6</td>\n",
              "      <td>103</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24.3</td>\n",
              "      <td>0.249</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>616</th>\n",
              "      <td>6</td>\n",
              "      <td>117</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28.7</td>\n",
              "      <td>0.157</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>648</th>\n",
              "      <td>11</td>\n",
              "      <td>136</td>\n",
              "      <td>84</td>\n",
              "      <td>35</td>\n",
              "      <td>130</td>\n",
              "      <td>28.3</td>\n",
              "      <td>0.260</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>678</th>\n",
              "      <td>3</td>\n",
              "      <td>121</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.127</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>3</td>\n",
              "      <td>162</td>\n",
              "      <td>52</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>37.2</td>\n",
              "      <td>0.652</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>3</td>\n",
              "      <td>158</td>\n",
              "      <td>76</td>\n",
              "      <td>36</td>\n",
              "      <td>245</td>\n",
              "      <td>31.6</td>\n",
              "      <td>0.851</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.391</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>5</td>\n",
              "      <td>124</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.220</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>2</td>\n",
              "      <td>81</td>\n",
              "      <td>60</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>27.7</td>\n",
              "      <td>0.290</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>3</td>\n",
              "      <td>115</td>\n",
              "      <td>66</td>\n",
              "      <td>39</td>\n",
              "      <td>140</td>\n",
              "      <td>38.1</td>\n",
              "      <td>0.150</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>692</th>\n",
              "      <td>2</td>\n",
              "      <td>121</td>\n",
              "      <td>70</td>\n",
              "      <td>32</td>\n",
              "      <td>95</td>\n",
              "      <td>39.1</td>\n",
              "      <td>0.886</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>689</th>\n",
              "      <td>1</td>\n",
              "      <td>144</td>\n",
              "      <td>82</td>\n",
              "      <td>46</td>\n",
              "      <td>180</td>\n",
              "      <td>46.1</td>\n",
              "      <td>0.335</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>683</th>\n",
              "      <td>4</td>\n",
              "      <td>125</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32.3</td>\n",
              "      <td>0.536</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>2</td>\n",
              "      <td>115</td>\n",
              "      <td>64</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>30.8</td>\n",
              "      <td>0.421</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>715</th>\n",
              "      <td>7</td>\n",
              "      <td>187</td>\n",
              "      <td>50</td>\n",
              "      <td>33</td>\n",
              "      <td>392</td>\n",
              "      <td>33.9</td>\n",
              "      <td>0.826</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>72</td>\n",
              "      <td>12</td>\n",
              "      <td>70</td>\n",
              "      <td>25.3</td>\n",
              "      <td>0.658</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>2</td>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.101</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>1</td>\n",
              "      <td>80</td>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19.1</td>\n",
              "      <td>0.258</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>9</td>\n",
              "      <td>123</td>\n",
              "      <td>70</td>\n",
              "      <td>44</td>\n",
              "      <td>94</td>\n",
              "      <td>33.1</td>\n",
              "      <td>0.374</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>1</td>\n",
              "      <td>139</td>\n",
              "      <td>46</td>\n",
              "      <td>19</td>\n",
              "      <td>83</td>\n",
              "      <td>28.7</td>\n",
              "      <td>0.654</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>1</td>\n",
              "      <td>147</td>\n",
              "      <td>94</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>49.3</td>\n",
              "      <td>0.358</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0</td>\n",
              "      <td>95</td>\n",
              "      <td>85</td>\n",
              "      <td>25</td>\n",
              "      <td>36</td>\n",
              "      <td>37.4</td>\n",
              "      <td>0.247</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>3</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.5</td>\n",
              "      <td>0.187</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>13</td>\n",
              "      <td>126</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43.4</td>\n",
              "      <td>0.583</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>0</td>\n",
              "      <td>117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33.8</td>\n",
              "      <td>0.932</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>545</th>\n",
              "      <td>8</td>\n",
              "      <td>186</td>\n",
              "      <td>90</td>\n",
              "      <td>35</td>\n",
              "      <td>225</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0.423</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548</th>\n",
              "      <td>1</td>\n",
              "      <td>164</td>\n",
              "      <td>82</td>\n",
              "      <td>43</td>\n",
              "      <td>67</td>\n",
              "      <td>32.8</td>\n",
              "      <td>0.341</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>10</td>\n",
              "      <td>115</td>\n",
              "      <td>98</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1.022</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>4</td>\n",
              "      <td>95</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.161</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>118</td>\n",
              "      <td>84</td>\n",
              "      <td>47</td>\n",
              "      <td>230</td>\n",
              "      <td>45.8</td>\n",
              "      <td>0.551</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>6</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.7</td>\n",
              "      <td>0.190</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>3</td>\n",
              "      <td>113</td>\n",
              "      <td>44</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>22.4</td>\n",
              "      <td>0.140</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>6</td>\n",
              "      <td>93</td>\n",
              "      <td>50</td>\n",
              "      <td>30</td>\n",
              "      <td>64</td>\n",
              "      <td>28.7</td>\n",
              "      <td>0.356</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>3</td>\n",
              "      <td>120</td>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "      <td>135</td>\n",
              "      <td>42.9</td>\n",
              "      <td>0.452</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "      <td>20</td>\n",
              "      <td>23</td>\n",
              "      <td>27.7</td>\n",
              "      <td>0.299</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>2</td>\n",
              "      <td>108</td>\n",
              "      <td>62</td>\n",
              "      <td>10</td>\n",
              "      <td>278</td>\n",
              "      <td>25.3</td>\n",
              "      <td>0.881</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>752</th>\n",
              "      <td>3</td>\n",
              "      <td>108</td>\n",
              "      <td>62</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.223</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>5</td>\n",
              "      <td>44</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.587</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>2</td>\n",
              "      <td>87</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>28.9</td>\n",
              "      <td>0.773</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>65</td>\n",
              "      <td>26</td>\n",
              "      <td>130</td>\n",
              "      <td>42.6</td>\n",
              "      <td>0.431</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>8</td>\n",
              "      <td>155</td>\n",
              "      <td>62</td>\n",
              "      <td>26</td>\n",
              "      <td>495</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.543</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>70</td>\n",
              "      <td>24</td>\n",
              "      <td>110</td>\n",
              "      <td>24.3</td>\n",
              "      <td>0.221</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25.1</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>8</td>\n",
              "      <td>181</td>\n",
              "      <td>68</td>\n",
              "      <td>36</td>\n",
              "      <td>495</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.615</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>8</td>\n",
              "      <td>109</td>\n",
              "      <td>76</td>\n",
              "      <td>39</td>\n",
              "      <td>114</td>\n",
              "      <td>27.9</td>\n",
              "      <td>0.640</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>197</td>\n",
              "      <td>70</td>\n",
              "      <td>45</td>\n",
              "      <td>543</td>\n",
              "      <td>30.5</td>\n",
              "      <td>0.158</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>645</th>\n",
              "      <td>2</td>\n",
              "      <td>157</td>\n",
              "      <td>74</td>\n",
              "      <td>35</td>\n",
              "      <td>440</td>\n",
              "      <td>39.4</td>\n",
              "      <td>0.134</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>0</td>\n",
              "      <td>105</td>\n",
              "      <td>68</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.236</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>7</td>\n",
              "      <td>109</td>\n",
              "      <td>80</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>35.9</td>\n",
              "      <td>1.127</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>231 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  BloodPressure  ...   BMI  DiabetesPedigreeFunction  Age\n",
              "766            1      126             60  ...  30.1                     0.349   47\n",
              "748            3      187             70  ...  36.4                     0.408   36\n",
              "42             7      106             92  ...  22.7                     0.235   48\n",
              "485            0      135             68  ...  42.3                     0.365   24\n",
              "543            4       84             90  ...  39.5                     0.159   25\n",
              "445            0      180             78  ...  59.4                     2.420   25\n",
              "427            1      181             64  ...  34.1                     0.328   38\n",
              "352            3       61             82  ...  34.4                     0.243   46\n",
              "428            0      135             94  ...  40.6                     0.284   26\n",
              "123            5      132             80  ...  26.8                     0.186   69\n",
              "525            3       87             60  ...  21.8                     0.444   21\n",
              "587            6      103             66  ...  24.3                     0.249   29\n",
              "616            6      117             96  ...  28.7                     0.157   30\n",
              "648           11      136             84  ...  28.3                     0.260   42\n",
              "678            3      121             52  ...  36.0                     0.127   25\n",
              "227            3      162             52  ...  37.2                     0.652   24\n",
              "31             3      158             76  ...  31.6                     0.851   28\n",
              "113            4       76             62  ...  34.0                     0.391   25\n",
              "116            5      124             74  ...  34.0                     0.220   38\n",
              "210            2       81             60  ...  27.7                     0.290   25\n",
              "318            3      115             66  ...  38.1                     0.150   28\n",
              "692            2      121             70  ...  39.1                     0.886   23\n",
              "689            1      144             82  ...  46.1                     0.335   46\n",
              "683            4      125             80  ...  32.3                     0.536   27\n",
              "423            2      115             64  ...  30.8                     0.421   21\n",
              "715            7      187             50  ...  33.9                     0.826   34\n",
              "446            1      100             72  ...  25.3                     0.658   28\n",
              "267            2      128             64  ...  40.0                     1.101   24\n",
              "90             1       80             55  ...  19.1                     0.258   21\n",
              "191            9      123             70  ...  33.1                     0.374   40\n",
              "..           ...      ...            ...  ...   ...                       ...  ...\n",
              "346            1      139             46  ...  28.7                     0.654   22\n",
              "746            1      147             94  ...  49.3                     0.358   27\n",
              "109            0       95             85  ...  37.4                     0.247   24\n",
              "347            3      116              0  ...  23.5                     0.187   23\n",
              "72            13      126             90  ...  43.4                     0.583   42\n",
              "336            0      117              0  ...  33.8                     0.932   44\n",
              "545            8      186             90  ...  34.5                     0.423   37\n",
              "548            1      164             82  ...  32.8                     0.341   50\n",
              "464           10      115             98  ...  24.0                     1.022   34\n",
              "400            4       95             64  ...  32.0                     0.161   31\n",
              "16             0      118             84  ...  45.8                     0.551   31\n",
              "601            6       96              0  ...  23.7                     0.190   28\n",
              "80             3      113             44  ...  22.4                     0.140   22\n",
              "98             6       93             50  ...  28.7                     0.356   23\n",
              "126            3      120             70  ...  42.9                     0.452   30\n",
              "182            1        0             74  ...  27.7                     0.299   21\n",
              "279            2      108             62  ...  25.3                     0.881   22\n",
              "752            3      108             62  ...  26.0                     0.223   25\n",
              "62             5       44             62  ...  25.0                     0.587   36\n",
              "172            2       87              0  ...  28.9                     0.773   25\n",
              "213            0      140             65  ...  42.6                     0.431   24\n",
              "111            8      155             62  ...  34.0                     0.543   46\n",
              "384            1      125             70  ...  24.3                     0.221   25\n",
              "268            0      102             52  ...  25.1                     0.078   21\n",
              "186            8      181             68  ...  30.1                     0.615   60\n",
              "188            8      109             76  ...  27.9                     0.640   31\n",
              "8              2      197             70  ...  30.5                     0.158   53\n",
              "645            2      157             74  ...  39.4                     0.134   30\n",
              "381            0      105             68  ...  20.0                     0.236   22\n",
              "314            7      109             80  ...  35.9                     1.127   43\n",
              "\n",
              "[231 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQtK0XSRgKHd",
        "colab_type": "code",
        "outputId": "4d89f7be-419c-4ec8-f3a0-87090a31091b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "train_test_split(y, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0      1\n",
              " 1      0\n",
              " 2      1\n",
              " 3      0\n",
              " 4      1\n",
              " 5      0\n",
              " 6      1\n",
              " 7      0\n",
              " 8      1\n",
              " 9      1\n",
              " 10     0\n",
              " 11     1\n",
              " 12     0\n",
              " 13     1\n",
              " 14     1\n",
              " 15     1\n",
              " 16     1\n",
              " 17     1\n",
              " 18     0\n",
              " 19     1\n",
              " 20     0\n",
              " 21     0\n",
              " 22     1\n",
              " 23     1\n",
              " 24     1\n",
              " 25     1\n",
              " 26     1\n",
              " 27     0\n",
              " 28     0\n",
              " 29     0\n",
              "       ..\n",
              " 546    1\n",
              " 547    0\n",
              " 548    0\n",
              " 549    0\n",
              " 550    0\n",
              " 551    0\n",
              " 552    0\n",
              " 553    0\n",
              " 554    0\n",
              " 555    0\n",
              " 556    0\n",
              " 557    0\n",
              " 558    0\n",
              " 559    0\n",
              " 560    1\n",
              " 561    1\n",
              " 562    0\n",
              " 563    0\n",
              " 564    0\n",
              " 565    0\n",
              " 566    0\n",
              " 567    0\n",
              " 568    0\n",
              " 569    1\n",
              " 570    0\n",
              " 571    0\n",
              " 572    0\n",
              " 573    0\n",
              " 574    0\n",
              " 575    0\n",
              " Name: Outcome, Length: 576, dtype: int64, 576    0\n",
              " 577    1\n",
              " 578    0\n",
              " 579    1\n",
              " 580    1\n",
              " 581    0\n",
              " 582    0\n",
              " 583    0\n",
              " 584    1\n",
              " 585    0\n",
              " 586    1\n",
              " 587    0\n",
              " 588    1\n",
              " 589    0\n",
              " 590    1\n",
              " 591    0\n",
              " 592    1\n",
              " 593    0\n",
              " 594    0\n",
              " 595    1\n",
              " 596    0\n",
              " 597    0\n",
              " 598    1\n",
              " 599    0\n",
              " 600    0\n",
              " 601    0\n",
              " 602    0\n",
              " 603    1\n",
              " 604    1\n",
              " 605    0\n",
              "       ..\n",
              " 738    0\n",
              " 739    1\n",
              " 740    1\n",
              " 741    0\n",
              " 742    0\n",
              " 743    1\n",
              " 744    0\n",
              " 745    0\n",
              " 746    1\n",
              " 747    0\n",
              " 748    1\n",
              " 749    1\n",
              " 750    1\n",
              " 751    0\n",
              " 752    0\n",
              " 753    1\n",
              " 754    1\n",
              " 755    1\n",
              " 756    0\n",
              " 757    1\n",
              " 758    0\n",
              " 759    1\n",
              " 760    0\n",
              " 761    1\n",
              " 762    0\n",
              " 763    0\n",
              " 764    0\n",
              " 765    0\n",
              " 766    1\n",
              " 767    0\n",
              " Name: Outcome, Length: 192, dtype: int64]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4r_FXsmZcgg",
        "colab_type": "text"
      },
      "source": [
        "**source**: https://www.kaggle.com/ranjeetjain3/visualization-machine-learning-deep-learning#-Description\n",
        "# **List of algorithms**\n",
        "Since it is a classification problem we will be using\n",
        "\n",
        "Logistic regression\n",
        "\n",
        "Decision tree\n",
        "\n",
        "KNN\n",
        "\n",
        "SVM\n",
        "\n",
        "Naive Bayes Classification\n",
        "\n",
        "Random forest\n",
        "\n",
        "XGBoost\n",
        "\n",
        "LightGBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4iQP_WxdT7B",
        "colab_type": "text"
      },
      "source": [
        "**Logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1aGjPQFdUIm",
        "colab_type": "code",
        "outputId": "45021794-6d44-4c54-8038-d4f1cd981a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(x_train,y_train)\n",
        "lr_predict = lr_model.predict(x_test)\n",
        "print('Logistic Regression - ',accuracy_score(lr_predict,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression -  0.7835497835497836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNFWNmFldUWB",
        "colab_type": "text"
      },
      "source": [
        "**“Support Vector Machine”** (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well.\n",
        "\n",
        "Support Vectors are simply the co-ordinates of individual observation. Support Vector Machine is a frontier which best segregates the two classes (hyper-plane/ line). (https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFglGoEKdUhk",
        "colab_type": "code",
        "outputId": "dc485be7-ac6d-454f-bdfb-2b942c628e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(x_train,y_train)\n",
        "svc_predict = svm_model.predict(x_test)\n",
        "print('SVM - ',accuracy_score(svc_predict,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM -  0.7662337662337663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtTMqCnDdUuq",
        "colab_type": "text"
      },
      "source": [
        "**Naive Bayes Classification**\n",
        "<img src = \"https://helloacm.com/wp-content/uploads/2016/03/Bayes_rule.png\">\n",
        "\n",
        "Naive Bayes is a simple, yet effective and commonly-used, machine learning classifier. It is a probabilistic classifier that makes classifications using the Maximum A Posteriori decision rule in a Bayesian setting. It can also be represented using a very simple Bayesian network. Naive Bayes classifiers have been especially popular for text classification, and are a traditional solution for problems such as spam detection. (https://towardsdatascience.com/introduction-to-naive-bayes-classification-4cffabb1ae54)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB5lcMrNdU6p",
        "colab_type": "code",
        "outputId": "8a5af42a-093d-4c6e-9ac3-969c9fc83326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nb_model = GaussianNB()\n",
        "nb_model.fit(x_train,y_train)\n",
        "nb_predict = nb_model.predict(x_test)\n",
        "print('Naive bayes - ',accuracy_score(nb_predict,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive bayes -  0.7619047619047619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQVqoFsZdVGg",
        "colab_type": "text"
      },
      "source": [
        "**Decision tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d6BB1gpdVS_",
        "colab_type": "code",
        "outputId": "739f9e89-943f-4389-8896-93ea418bafee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dt_model = DecisionTreeClassifier(max_leaf_nodes=3)\n",
        "dt_model.fit(x_train,y_train)\n",
        "dt_predict = dt_model.predict(x_test)\n",
        "print('Decision Tree - ',accuracy_score(dt_predict,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree -  0.7445887445887446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yDiSIsdVio",
        "colab_type": "text"
      },
      "source": [
        "**Random forest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yJ9XYDqdVs4",
        "colab_type": "code",
        "outputId": "54b6665c-0f60-4aba-b46d-d749cc823039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rfc_model = RandomForestClassifier(max_depth=3)\n",
        "rfc_model.fit(x_train,y_train)\n",
        "rfc_predict = rfc_model.predict(x_test)\n",
        "print('Random Forest - ',accuracy_score(rfc_predict,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest -  0.7662337662337663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHnGsU8Mfu6G",
        "colab_type": "text"
      },
      "source": [
        "**Extra Tree Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEiUEUlKfvE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "etc_model = ExtraTreesClassifier()\n",
        "etc_model.fit(x_train,y_train)\n",
        "etc_predict = etc_model.predict(x_test)\n",
        "print('Extra Tree Classifier - ',accuracy_score(etc_predict,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yra_ZnfnfvRT",
        "colab_type": "text"
      },
      "source": [
        "**KNN**\n",
        "\n",
        "\n",
        "K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970's as a non-parametric technique. (https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTmij3sZfvbs",
        "colab_type": "code",
        "outputId": "f208bcbf-3f67-4ecf-8bb9-6f6a979ad54d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_model.fit(x_train,y_train)\n",
        "knn_predict = knn_model.predict(x_test)\n",
        "print('knn - ',accuracy_score(knn_predict,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "knn -  0.70995670995671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIkO6pjqgI51",
        "colab_type": "text"
      },
      "source": [
        "**XGBoost**\n",
        "\n",
        "\n",
        "The beauty of this powerful algorithm lies in its scalability, which drives fast learning through parallel and distributed computing and offers efficient memory usage.\n",
        "\n",
        "It’s no wonder then that CERN recognized it as the best approach to classify signals from the Large Hadron Collider. This particular challenge posed by CERN required a solution that would be scalable to process data being generated at the rate of 3 petabytes per year and effectively distinguish an extremely rare signal from background noises in a complex physical process. XGBoost emerged as the most useful, straightforward and robust solution.\n",
        "\n",
        "(https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeSu0wfugBQ8",
        "colab_type": "code",
        "outputId": "c05d6e48-1550-4edf-9d57-22d4dd11d0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "xg_model = xgb.XGBClassifier()\n",
        "xg_model = xg_model.fit(x_train,y_train)\n",
        "xg_model.score(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7705627705627706"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I69hsZuKY60Z",
        "colab_type": "text"
      },
      "source": [
        "# **Prepare Train and Test**\n",
        "scikit-learn provides a helpful function for partitioning data, train_test_split, which splits out your data into a training set and a test set.\n",
        "\n",
        "Training set for fitting the model Test set for evaluation only\n",
        "\n",
        "The steps to building and using a model are:\n",
        "\n",
        "**Define**: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\n",
        "\n",
        "**Fit**: Capture patterns from provided data. This is the heart of modeling.\n",
        "\n",
        "**Predict**: Just what it sounds like\n",
        "\n",
        "**Evaluate**: Determine how accurate the model's predictions are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWVkQiVNWSTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = train[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]# taking the training data features\n",
        "train_y=train.Outcome# output of our training data\n",
        "test_X= test[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']] # taking test data features\n",
        "test_y =test.Outcome   #output value of test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSgoPwN2VtMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = data[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]\n",
        "y = data['Outcome']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csY3Y8yjG9t_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URZEIf16Y5zp",
        "colab_type": "text"
      },
      "source": [
        "# **List of 17 algorithms (train + test algo)**\n",
        "\n",
        "\n",
        "1.Decision Tree\n",
        "\n",
        "2.RandomForest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O38nBywPGsYJ",
        "colab_type": "text"
      },
      "source": [
        "# **1.Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPuuDkENDdoC",
        "colab_type": "code",
        "outputId": "a6984b33-e7e9-4278-d947-6adeafbafca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "\n",
        "# Decision Tree's\n",
        "from sklearn.tree import DecisionTreeClassifier # মডেল + মডেল ক্লাস\n",
        "\n",
        "Model = DecisionTreeClassifier()\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.87      0.81       149\n",
            "           1       0.68      0.51      0.58        82\n",
            "\n",
            "    accuracy                           0.74       231\n",
            "   macro avg       0.72      0.69      0.70       231\n",
            "weighted avg       0.73      0.74      0.73       231\n",
            "\n",
            "[[129  20]\n",
            " [ 40  42]]\n",
            "accuracy is 0.7402597402597403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEHZPRyUdJBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJs0RYE4P8yr",
        "colab_type": "text"
      },
      "source": [
        "# **2.RandomForest**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X02Ijv3QP88p",
        "colab_type": "code",
        "outputId": "78382d47-6ad2-45b3-e006-d4a9f638d2a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "Model=RandomForestClassifier(max_depth=2)\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.94      0.83       149\n",
            "           1       0.78      0.39      0.52        82\n",
            "\n",
            "    accuracy                           0.74       231\n",
            "   macro avg       0.76      0.66      0.67       231\n",
            "weighted avg       0.75      0.74      0.72       231\n",
            "\n",
            "[[140   9]\n",
            " [ 50  32]]\n",
            "accuracy is 0.7445887445887446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aI8LRRBIPOW",
        "colab_type": "text"
      },
      "source": [
        "# **3 Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bNQgTwMIPbH",
        "colab_type": "code",
        "outputId": "29052c5e-d7c6-40b2-ac67-f95e85ae8d48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "#LogisticRegression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "Model = LogisticRegression()\n",
        "#from sklearn import model_selection #another type of model\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.87      0.82       149\n",
            "           1       0.70      0.54      0.61        82\n",
            "\n",
            "    accuracy                           0.75       231\n",
            "   macro avg       0.74      0.70      0.71       231\n",
            "weighted avg       0.75      0.75      0.74       231\n",
            "\n",
            "[[130  19]\n",
            " [ 38  44]]\n",
            "accuracy is 0.7532467532467533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zymHcs-8IoGP",
        "colab_type": "text"
      },
      "source": [
        "# **4 K-Nearest Neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_zIwf2ZIodN",
        "colab_type": "code",
        "outputId": "46da4b6a-76af-468a-d713-adb7ea456f6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# K-Nearest Neighbours\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "Model = KNeighborsClassifier(n_neighbors=8)\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.92      0.84       149\n",
            "           1       0.78      0.51      0.62        82\n",
            "\n",
            "    accuracy                           0.77       231\n",
            "   macro avg       0.78      0.72      0.73       231\n",
            "weighted avg       0.78      0.77      0.76       231\n",
            "\n",
            "[[137  12]\n",
            " [ 40  42]]\n",
            "accuracy is 0.7748917748917749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70l5VoS2Iosn",
        "colab_type": "text"
      },
      "source": [
        "# **5 Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW9MUAOPIo3T",
        "colab_type": "code",
        "outputId": "f5e854cc-7196-4661-b11a-ac0f979d3ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "Model = GaussianNB()\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       149\n",
            "           1       0.75      0.65      0.69        82\n",
            "\n",
            "    accuracy                           0.80       231\n",
            "   macro avg       0.78      0.76      0.77       231\n",
            "weighted avg       0.79      0.80      0.79       231\n",
            "\n",
            "[[131  18]\n",
            " [ 29  53]]\n",
            "accuracy is 0.7965367965367965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv3RWpvLIpCI",
        "colab_type": "text"
      },
      "source": [
        "**6 SVM**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lElDz1HyIpNx",
        "colab_type": "code",
        "outputId": "d1a419b6-d850-4908-e93d-6e137e98abb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Support Vector Machine\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "Model = SVC()\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      1.00      0.78       149\n",
            "           1       0.00      0.00      0.00        82\n",
            "\n",
            "    accuracy                           0.65       231\n",
            "   macro avg       0.32      0.50      0.39       231\n",
            "weighted avg       0.42      0.65      0.51       231\n",
            "\n",
            "[[149   0]\n",
            " [ 82   0]]\n",
            "accuracy is 0.645021645021645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxFQ5hVJIpao",
        "colab_type": "text"
      },
      "source": [
        "# **7 Nu-Support Vector Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0e0UnWDIpmX",
        "colab_type": "code",
        "outputId": "d9d6546b-f65b-43ac-a254-2519baeb983b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Support Vector Machine's \n",
        "from sklearn.svm import NuSVC\n",
        "\n",
        "ModelNU = NuSVC()\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      1.00      0.78       149\n",
            "           1       0.00      0.00      0.00        82\n",
            "\n",
            "    accuracy                           0.65       231\n",
            "   macro avg       0.32      0.50      0.39       231\n",
            "weighted avg       0.42      0.65      0.51       231\n",
            "\n",
            "[[149   0]\n",
            " [ 82   0]]\n",
            "accuracy is 0.645021645021645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjGC9G3uIpz7",
        "colab_type": "text"
      },
      "source": [
        "# **8 Linear Support Vector Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h51zxYOjIp_P",
        "colab_type": "code",
        "outputId": "5a52a312-85a7-418f-fde7-8211f8b8808a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Linear Support Vector Classification\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "Model = LinearSVC()\n",
        "\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.17      0.28       149\n",
            "           1       0.39      0.98      0.56        82\n",
            "\n",
            "    accuracy                           0.45       231\n",
            "   macro avg       0.66      0.57      0.42       231\n",
            "weighted avg       0.74      0.45      0.38       231\n",
            "\n",
            "[[ 25 124]\n",
            " [  2  80]]\n",
            "accuracy is 0.45454545454545453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD08znYJT0sd",
        "colab_type": "text"
      },
      "source": [
        "  \n",
        "# **Linear Discriminant Analysis**\n",
        "\n",
        "(discriminant_analysis.LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (discriminant_analysis.QuadraticDiscriminantAnalysis) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.\n",
        "\n",
        "These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyperparameters to tune."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auoolevaT1BR",
        "colab_type": "code",
        "outputId": "4dce1867-627c-4cc0-82e5-5de6c4e28442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        " \n",
        "#from sklearn.linear_model import LinearDiscriminantAnalysis\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "Model=LinearDiscriminantAnalysis()\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "#Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.93      0.85       149\n",
            "           1       0.81      0.56      0.66        82\n",
            "\n",
            "    accuracy                           0.80       231\n",
            "   macro avg       0.80      0.74      0.76       231\n",
            "weighted avg       0.80      0.80      0.79       231\n",
            "\n",
            "[[138  11]\n",
            " [ 36  46]]\n",
            "accuracy is 0.7965367965367965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1kWX7zEIqM3",
        "colab_type": "text"
      },
      "source": [
        "# **9 Radius Neighbors Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGTrULU1IqYX",
        "colab_type": "code",
        "outputId": "6474cab0-419c-4d06-b6cb-5cfd2fa4e8b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "from sklearn.neighbors import  RadiusNeighborsClassifier\n",
        "Model=RadiusNeighborsClassifier(radius=148)\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.99      0.78       149\n",
            "           1       0.60      0.04      0.07        82\n",
            "\n",
            "    accuracy                           0.65       231\n",
            "   macro avg       0.63      0.51      0.43       231\n",
            "weighted avg       0.63      0.65      0.53       231\n",
            "\n",
            "[[147   2]\n",
            " [ 79   3]]\n",
            "accuracy is 0.6493506493506493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snh-Y74sKH9B",
        "colab_type": "text"
      },
      "source": [
        "# **10 Passive Aggressive Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6-qK4Q8KINU",
        "colab_type": "code",
        "outputId": "ab8fa35e-2911-438d-a6b1-485114039930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "Model = PassiveAggressiveClassifier()\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.01      0.01       149\n",
            "           1       0.36      1.00      0.53        82\n",
            "\n",
            "    accuracy                           0.36       231\n",
            "   macro avg       0.68      0.50      0.27       231\n",
            "weighted avg       0.77      0.36      0.20       231\n",
            "\n",
            "[[  1 148]\n",
            " [  0  82]]\n",
            "accuracy is 0.3593073593073593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkUqXxXNKIg6",
        "colab_type": "text"
      },
      "source": [
        "# **11 BernoulliNB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsUnj6ctKIp3",
        "colab_type": "code",
        "outputId": "e48d3f83-9265-48bf-ddd8-6069bd91f988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# BernoulliNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "Model = BernoulliNB()\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.99      0.78       149\n",
            "           1       0.00      0.00      0.00        82\n",
            "\n",
            "    accuracy                           0.64       231\n",
            "   macro avg       0.32      0.49      0.39       231\n",
            "weighted avg       0.41      0.64      0.50       231\n",
            "\n",
            "[[147   2]\n",
            " [ 82   0]]\n",
            "accuracy is 0.6363636363636364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTLgXUUCKIzf",
        "colab_type": "text"
      },
      "source": [
        "# **12 ExtraTreeClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUSMI_4qKI-k",
        "colab_type": "code",
        "outputId": "97003115-a794-45d9-bcde-a604d4ad90ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# ExtraTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "\n",
        "Model = ExtraTreeClassifier()\n",
        "\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.77      0.73       149\n",
            "           1       0.47      0.37      0.41        82\n",
            "\n",
            "    accuracy                           0.63       231\n",
            "   macro avg       0.58      0.57      0.57       231\n",
            "weighted avg       0.61      0.63      0.62       231\n",
            "\n",
            "[[115  34]\n",
            " [ 52  30]]\n",
            "accuracy is 0.6277056277056277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZqgNWZxKJKI",
        "colab_type": "text"
      },
      "source": [
        "# **13 Bagging classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KYz9x2CKJUo",
        "colab_type": "code",
        "outputId": "14f75bf6-3926-4a9b-eef6-9b2d059b32df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "Model=BaggingClassifier()\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.89      0.84       149\n",
            "           1       0.74      0.56      0.64        82\n",
            "\n",
            "    accuracy                           0.77       231\n",
            "   macro avg       0.76      0.73      0.74       231\n",
            "weighted avg       0.77      0.77      0.77       231\n",
            "\n",
            "[[133  16]\n",
            " [ 36  46]]\n",
            "accuracy is 0.7748917748917749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaGmRJWoKJfh",
        "colab_type": "text"
      },
      "source": [
        "# **14 AdaBoost classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpryVNq9KJrV",
        "colab_type": "code",
        "outputId": "baa710b5-84fe-483b-da83-0b888fd4f98c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "Model=AdaBoostClassifier()\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.83       149\n",
            "           1       0.73      0.59      0.65        82\n",
            "\n",
            "    accuracy                           0.77       231\n",
            "   macro avg       0.76      0.73      0.74       231\n",
            "weighted avg       0.77      0.77      0.77       231\n",
            "\n",
            "[[131  18]\n",
            " [ 34  48]]\n",
            "accuracy is 0.7748917748917749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-xvYbc6KJ23",
        "colab_type": "text"
      },
      "source": [
        "# **15 Gradient Boosting Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZJdacvxKKCO",
        "colab_type": "code",
        "outputId": "dec519b7-a183-490d-a509-42d499e04864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "ModelG=GradientBoostingClassifier()\n",
        "\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.83       149\n",
            "           1       0.73      0.59      0.65        82\n",
            "\n",
            "    accuracy                           0.77       231\n",
            "   macro avg       0.76      0.73      0.74       231\n",
            "weighted avg       0.77      0.77      0.77       231\n",
            "\n",
            "[[131  18]\n",
            " [ 34  48]]\n",
            "accuracy is 0.7748917748917749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BxT4vmEKKMd",
        "colab_type": "text"
      },
      "source": [
        "# **16 Linear Discriminant Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6dBihgjKKXf",
        "colab_type": "code",
        "outputId": "6964a907-b28e-44b6-ff10-1bfb1f5a3538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "Model=LinearDiscriminantAnalysis()\n",
        "\n",
        "Model.fit(train_X, train_y)\n",
        "\n",
        "y_predL = Model.predict(test_X)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.93      0.85       149\n",
            "           1       0.81      0.56      0.66        82\n",
            "\n",
            "    accuracy                           0.80       231\n",
            "   macro avg       0.80      0.74      0.76       231\n",
            "weighted avg       0.80      0.80      0.79       231\n",
            "\n",
            "[[138  11]\n",
            " [ 36  46]]\n",
            "accuracy is 0.7965367965367965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxarZZCkKKhu",
        "colab_type": "text"
      },
      "source": [
        "# **17 Quadratic Discriminant Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmwWii3EKKtO",
        "colab_type": "code",
        "outputId": "0ca8020f-e899-4318-e49b-36529d41b5cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        " \n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "Model=QuadraticDiscriminantAnalysis()\n",
        "\n",
        "Model.fit(train_X, train_y) #ডাটাকে সাজাই ফীচার ম্যাট্রিক্স এবং টার্গেট ভেক্টরের মতো করে।\n",
        "y_predL = Model.predict(test_X) #নতুন ডাটার লেবেল প্রেডিক্ট করা।\n",
        "\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.87      0.82       149\n",
            "           1       0.70      0.54      0.61        82\n",
            "\n",
            "    accuracy                           0.75       231\n",
            "   macro avg       0.74      0.70      0.71       231\n",
            "weighted avg       0.75      0.75      0.74       231\n",
            "\n",
            "[[130  19]\n",
            " [ 38  44]]\n",
            "accuracy is 0.7532467532467533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Iz4RacHPa5b",
        "colab_type": "text"
      },
      "source": [
        "# **18.Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMw1pa43M8zb",
        "colab_type": "code",
        "outputId": "36055497-a2ba-4043-caa8-871d42405d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "#from sklearn import model_selection\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "#Model=QuadraticDiscriminantAnalysis()\n",
        " \n",
        "#from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "Model.fit(train_X, train_y) #ডাটাকে সাজাই ফীচার ম্যাট্রিক্স এবং টার্গেট ভেক্টরের মতো করে।\n",
        "y_predL = Model.predict(test_X) #নতুন ডাটার লেবেল প্রেডিক্ট করা।\n",
        "\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.89      0.83       149\n",
            "           1       0.73      0.56      0.63        82\n",
            "\n",
            "    accuracy                           0.77       231\n",
            "   macro avg       0.76      0.72      0.73       231\n",
            "weighted avg       0.77      0.77      0.76       231\n",
            "\n",
            "[[132  17]\n",
            " [ 36  46]]\n",
            "accuracy is 0.7705627705627706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0z9cn7sUvrC",
        "colab_type": "text"
      },
      "source": [
        "# **19 Quadratic Discriminant Analysis**\n",
        "\n",
        "A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.\n",
        "\n",
        "The model fits a Gaussian density to each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUmvIPpbSbSy",
        "colab_type": "code",
        "outputId": "0c762af3-20a2-4b2e-e2ac-aa02c3cc4548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        " \n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "Model=QuadraticDiscriminantAnalysis()\n",
        "\n",
        "Model.fit(train_X, train_y) #ডাটাকে সাজাই ফীচার ম্যাট্রিক্স এবং টার্গেট ভেক্টরের মতো করে।\n",
        "y_predL = Model.predict(test_X) #নতুন ডাটার লেবেল প্রেডিক্ট করা।\n",
        "\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "print(classification_report(test_y, y_predL))\n",
        "print(confusion_matrix(test_y, y_predL))\n",
        "# Accuracy score\n",
        "print('accuracy is',accuracy_score(y_predL,test_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.87      0.82       149\n",
            "           1       0.70      0.54      0.61        82\n",
            "\n",
            "    accuracy                           0.75       231\n",
            "   macro avg       0.74      0.70      0.71       231\n",
            "weighted avg       0.75      0.75      0.74       231\n",
            "\n",
            "[[130  19]\n",
            " [ 38  44]]\n",
            "accuracy is 0.7532467532467533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMdPb9jnMYNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTJfmNICLqXy",
        "colab_type": "code",
        "outputId": "c737c39d-99ef-4502-a964-415c2b91a4d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "models = pd.DataFrame({\n",
        "    'Model': [\n",
        "        'Decision Tree', 'Random Forest',\n",
        "              'LogisticRegression','K-Nearest Neighbours', 'Naive Bayes', 'SVM', 'Nu-Support Vector Classification',\n",
        "             'Linear Support Vector Classification', 'Radius Neighbors Classifier', 'Passive Aggressive Classifier','BernoulliNB',\n",
        "             'ExtraTreeClassifier', \"Bagging classifier \", \"AdaBoost classifier\", 'Gradient Boosting Classifier' ,'Linear Discriminant Analysis',\n",
        "             'Quadratic Discriminant Analysis'],\n",
        "    'Score': [DT, RT,LR, KNN,NBB,SVMm, NuS,  LSVM , RNC, PAC, Ber, ETC, BCC, AdaB,  GBCC, LDAA, QDAx]})\n",
        "models.sort_values(by='Score', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-8bd6168974da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m              \u001b[0;34m'ExtraTreeClassifier'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Bagging classifier \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AdaBoost classifier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gradient Boosting Classifier'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'Linear Discriminant Analysis'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m              'Quadratic Discriminant Analysis'],\n\u001b[0;32m----> 9\u001b[0;31m     'Score': [DT, RT,LR, KNN,NBB,SVMm, NuS,  LSVM , RNC, PAC, Ber, ETC, BCC, AdaB,  GBCC, LDAA, QDAx]})\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DT' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfJcQkwAe43P",
        "colab_type": "text"
      },
      "source": [
        "# **Compare Algorithms**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au3XkHev1mw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX34Y-_wdxJW",
        "colab_type": "code",
        "outputId": "9019142b-01e5-47c5-c7af-fd9980a8c1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "# prepare models\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "\tkfold = model_selection.KFold(n_splits=10)\n",
        "\tcv_results = model_selection.cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
        "\tresults.append(cv_results)\n",
        "\tnames.append(name)\n",
        "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "\tprint(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR: 0.769515 (0.048411)\n",
            "LDA: 0.773462 (0.051592)\n",
            "KNN: 0.726555 (0.061821)\n",
            "CART: 0.697779 (0.071275)\n",
            "NB: 0.755178 (0.042766)\n",
            "SVM: 0.651025 (0.072141)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/weoZwom8bVAdgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18_eKhro1o5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
